OJ
# a)
trainIndicies <- sample(nrow(OJ), 800)
train <- OJ[trainIndicies,]
test <- OJ[-trainIndicies,]
# b)
library(e1071)
svmModel <- svm(Purchase~., train, kernel="linear", cost=0.01)
summary(svmModel)
# c)
predTrain <- predict(svmModel, train)
predTest <- predict(svmModel, test)
trainError1 <- sum(predTrain!=train$Purchase)/nrow(train)
testError1 <- sum(predTest!=test$Purchase)/nrow(test)
trainError1
testError1
# d)
costBest <- tune(svm, Purchase~., data=train, kernel="linear", ranges = list(cost=c(0.01, 0.1, 1, 5, 10)))
costBest
# e)
newSvmModel <- svm(Purchase~., train, kernel="linear", cost=10)
trainError2 <- mean(predict(newSvmModel, train)!=train$Purchase)
testError2 <- mean(predict(newSvmModel, test)!=test$Purchase)
trainError2
testError2
# f)
svmRad <- svm(Purchase~., train, kernel="radial", cost=0.01)
summary(svmRad)
trainErrRad <- sum(predict(svmRad, train)!=train$Purchase)/nrow(train)
testErrRad <- sum(predict(svmRad, test)!=test$Purchase)/nrow(test)
costBestRad  <-  tune(svm, Purchase~., data=train, kernel="radial", ranges = list(cost=c(0.01, 0.1, 1, 5, 10)))
svmRadNew <- svm(Purchase~., data=train, kernel="radial", cost=1)
trainErrRadNew <- mean(predict(svmRadNew, train)!=train$Purchase)
testErrRadNew <- mean(predict(svmRadNew, test)!=test$Purchase)
trainErrRad
testErrRad
trainErrRadNew
testErrRadNew
#g)
svmPoly <- svm(Purchase~., train, kernel = "polynomial", degree=2, cost=0.01)
summary(svmPoly)
trainErrPoly <- mean(predict(svmPoly, train)!=train$Purchase)
testErrPoly <- mean(predict(svmPoly, test)!=test$Purchase)
costBestPoly <- tune(svm, Purchase~., data=train, kernel="polynomial", degree=2, ranges=list(cost=c(0.01, 0.1, 1, 5, 10)))
svmPolyNew <- svm(Purchase~., train, kernel="polynomial", degree=2, cost=10)
trainErrPolyNew <- mean(predict(svmPolyNew, train)!=train$Purchase)
testErrPolyNew <- mean(predict(svmPolyNew, test)!=test$Purchase)
trainErrPoly
testErrPoly
trainErrPolyNew
testErrPolyNew
# h)
print(paste("Linear Kernel Test Misclassification Error Original:", testError1))
print(paste("Linear Kernel Test Misclassification Error Tuned:", testError2))
print(paste("Radial Kernel Test Misclassification Error Original:", testErrRad))
print(paste("Radial Kernel Test Misclassification Error Tuned:", testErrRadNew))
print(paste("Polynomial Kernel Test Misclassification Error Original:", testErrPoly))
print(paste("Polynomial Kernel Test Misclassification Error Tuned:", testErrPolyNew))
# The linear kernel with the tuned cost parameter has the best results since it has the lowest test misclassification error
library(kernlab)
data(spam)
# spam
library(randomForest)
trainTreeIndex <- sample(seq_len(nrow(spam)), 0.7*nrow(spam))
trainTree <- spam[trainTreeIndex,]
testTree <- spam[-trainTreeIndex,]
oobError <- rep(0, 10)
testError <- rep(0, 10)
for(m in 1:10){
rfModel <- randomForest(type~., data = trainTree, mtry=m, ntree=25)
oobError[m] <- mean(rfModel$err.rate[,1])
rfPred <- predict(rfModel, testTree)
testError[m] <- mean(rfPred != testTree$type)
}
m <- 1:10
plot(m, oobError, xlim=c(0,10), ylim=c(0,0.25), main="Error for Different # Parameters", ylab="Error")
points(m, testError, col="red")
legend("topright",legend = c("OOB Error", "Test Error"), col=c("black", "red"), lty=1)
rfModel$err.rate
library(kernlab)
data(spam)
# spam
library(randomForest)
trainTreeIndex <- sample(seq_len(nrow(spam)), 0.7*nrow(spam))
trainTree <- spam[trainTreeIndex,]
testTree <- spam[-trainTreeIndex,]
oobError <- rep(0, 10)
testError <- rep(0, 10)
for(m in 1:10){
rfModel <- randomForest(type~., data = trainTree, mtry=m, ntree=25)
oobError[m] <- mean(rfModel$err.rate[,1])
rfPred <- predict(rfModel, testTree)
testError[m] <- mean(rfPred != testTree$type)
}
m <- 1:10
plot(m, oobError, xlim=c(0,10), ylim=c(0,0.25), main="Error for Different # Parameters", ylab="Error")
points(m, testError, col="red")
legend("topright",legend = c("OOB Error", "Test Error"), col=c("black", "red"), lty=1)
trainTree
costBest <- tune(svm, Purchase~., data=train, kernel="linear", ranges = list(cost=seq(0,10, 0.1)))
seq(0,10, 0.1)
costBest <- tune(svm, Purchase~., data=train, kernel="linear", ranges = cost=seq(0,10, 0.1))
# d)
costBest <- tune(svm, Purchase~., data=train, kernel="linear", ranges = list(cost=seq(0,10, 0.1)))
costBest <- tune(svm, Purchase~., data=train, kernel="linear", ranges = list(cost=seq(0,10, 0.1)))
set.seed(1)
library(ISLR2)
OJ
# a)
trainIndicies <- sample(nrow(OJ), 800)
train <- OJ[trainIndicies,]
test <- OJ[-trainIndicies,]
# b)
library(e1071)
svmModel <- svm(Purchase~., train, kernel="linear", cost=0.01)
summary(svmModel)
# c)
predTrain <- predict(svmModel, train)
predTest <- predict(svmModel, test)
trainError1 <- sum(predTrain!=train$Purchase)/nrow(train)
testError1 <- sum(predTest!=test$Purchase)/nrow(test)
trainError1
testError1
# d)
costBest <- tune(svm, Purchase~., data=train, kernel="linear", ranges = list(cost=seq(0,10, 0.1)))
costBest <- tune(svm, Purchase~., data=train, kernel="linear", ranges = list(cost=seq(0.01,10, 0.1)))
costBest
set.seed(1)
library(ISLR2)
OJ
# a)
trainIndicies <- sample(nrow(OJ), 800)
train <- OJ[trainIndicies,]
test <- OJ[-trainIndicies,]
# b)
library(e1071)
svmModel <- svm(Purchase~., train, kernel="linear", cost=0.01)
summary(svmModel)
# c)
predTrain <- predict(svmModel, train)
predTest <- predict(svmModel, test)
trainError1 <- sum(predTrain!=train$Purchase)/nrow(train)
testError1 <- sum(predTest!=test$Purchase)/nrow(test)
trainError1
testError1
# d)
costBest <- tune(svm, Purchase~., data=train, kernel="linear", ranges = list(cost=seq(0.01,10, 0.1)))
costBest
# e)
newSvmModel <- svm(Purchase~., train, kernel="linear", cost=10)
trainError2 <- mean(predict(newSvmModel, train)!=train$Purchase)
testError2 <- mean(predict(newSvmModel, test)!=test$Purchase)
trainError2
testError2
# f)
svmRad <- svm(Purchase~., train, kernel="radial", cost=0.01)
summary(svmRad)
trainErrRad <- sum(predict(svmRad, train)!=train$Purchase)/nrow(train)
testErrRad <- sum(predict(svmRad, test)!=test$Purchase)/nrow(test)
costBestRad  <-  tune(svm, Purchase~., data=train, kernel="radial", ranges = list(cost=c(0.01, 0.1, 1, 5, 10)))
svmRadNew <- svm(Purchase~., data=train, kernel="radial", cost=1)
trainErrRadNew <- mean(predict(svmRadNew, train)!=train$Purchase)
testErrRadNew <- mean(predict(svmRadNew, test)!=test$Purchase)
trainErrRad
testErrRad
trainErrRadNew
testErrRadNew
#g)
svmPoly <- svm(Purchase~., train, kernel = "polynomial", degree=2, cost=0.01)
summary(svmPoly)
trainErrPoly <- mean(predict(svmPoly, train)!=train$Purchase)
testErrPoly <- mean(predict(svmPoly, test)!=test$Purchase)
costBestPoly <- tune(svm, Purchase~., data=train, kernel="polynomial", degree=2, ranges=list(cost=c(0.01, 0.1, 1, 5, 10)))
svmPolyNew <- svm(Purchase~., train, kernel="polynomial", degree=2, cost=10)
trainErrPolyNew <- mean(predict(svmPolyNew, train)!=train$Purchase)
testErrPolyNew <- mean(predict(svmPolyNew, test)!=test$Purchase)
trainErrPoly
testErrPoly
trainErrPolyNew
testErrPolyNew
# h)
print(paste("Linear Kernel Test Misclassification Error Original:", testError1))
print(paste("Linear Kernel Test Misclassification Error Tuned:", testError2))
print(paste("Radial Kernel Test Misclassification Error Original:", testErrRad))
print(paste("Radial Kernel Test Misclassification Error Tuned:", testErrRadNew))
print(paste("Polynomial Kernel Test Misclassification Error Original:", testErrPoly))
print(paste("Polynomial Kernel Test Misclassification Error Tuned:", testErrPolyNew))
# The linear kernel with the tuned cost parameter has the best results since it has the lowest test misclassification error
svmRad <- svm(Purchase~., train, kernel="radial", cost=0.01)
summary(svmRad)
trainErrRad <- sum(predict(svmRad, train)!=train$Purchase)/nrow(train)
testErrRad <- sum(predict(svmRad, test)!=test$Purchase)/nrow(test)
costBestRad  <-  tune(svm, Purchase~., data=train, kernel="radial", ranges = list(cost=c(0.01, 0.1, 1, 5, 10)))
svmRadNew <- svm(Purchase~., data=train, kernel="radial", cost=1)
trainErrRadNew <- mean(predict(svmRadNew, train)!=train$Purchase)
testErrRadNew <- mean(predict(svmRadNew, test)!=test$Purchase)
trainErrRad
testErrRad
trainErrRadNew
testErrRadNew
costBestRad
setwd("C:/Users/bchan/OneDrive - Wilfrid Laurier University/ST494/Statistical-Learning")
git pull
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(e1071)
library(MASS)
library(tidyverse)
library(caret)
library(leaps)
select <- dplyr::select
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(e1071)
library(MASS)
library(tidyverse)
library(caret)
library(leaps)
library(tidyr)
library(ggplot2)
select <- dplyr::select
contents <- tribble(
~Index, ~Category, ~Method,
1, "Introduction", "Written",
2, "Preprocessing", "Data Description and Encoding",
3, "Unsupervised Learning/Analysis", "PCA and Clustering",
4, "Preprocessing", "Variable Selection",
5, "Supervised Regression", "Linear and Non-Linear Regression",
6, "Supervised Classification", "LDA/QDA and Logistic Regression",
7, "Supervised Classification", "Tree-Based Methods",
8, "Supervised Classification", "Support Vector Machines",
9, "Deep Learning", "Neural Networks",
10, "Conclusion", "Written",
)
contents
location = "https://raw.githubusercontent.com/BrandonYChan/Statistical-Learning/main/Student_performance_10k.csv"
df <- read_csv(location, show_col_types = FALSE)
#NOTE: GPA IS AVERAGE OF 4 courses, FOR MODEL THUS CANT USE ALL 4 AT ONCE
df
df2 <- df |>
rename(
"has_subsidized_lunch" = "lunch",
"has_prepared" = "test_preparation_course",
"is_male" = "gender",
"parent_education" = "parental_level_of_education",
"gpa" = "total_score"
) |>
mutate(
is_male = as.integer(if_else(is_male =="male",1,0)),
race_ethnicity = str_sub(df$race_ethnicity, -1, -1),
gpa = gpa/4
) |>
select(-c("roll_no",
"science_score",
"writing_score",    #REMOVE FOR NOW!!!!!!!!!!!!!!!!!!!!!!!!!,
"grade"
))
#Remove Any Duplicates
df3 <- distinct(df2)
#Replace Missing NUMERIC (Including Date) Values With ColMean
if (any(is.na(df3))) {
df3[] <- lapply(df3, function(x) {
if (is.numeric(x)) {
replace(x, is.na(x), mean(x, na.rm = TRUE))
} else {
x
}
})
}
#Dropping Missing Factor Columns
df3 <- df3[complete.cases(df3[c("race_ethnicity", "parent_education")]), ]
#For Regression We Convert All Categories Into ONEHOT Notation
df_reg <- df3 |>
bind_cols(as.data.frame(model.matrix(~ `race_ethnicity` + `parent_education` - 1, data = df3))) |>
select(-c("race_ethnicity","parent_education"))
df_cls <- df3 |>
mutate(
race_ethnicity = as.factor(race_ethnicity),
parent_education = as.factor(parent_education)
)
df_numeric <- df_cls |>
select(where(is.numeric)) |>
pivot_longer(cols = everything(), names_to = "variable", values_to = "value")
ggplot(df_numeric, aes(x = value)) +
geom_histogram(bins = 30) +
facet_wrap(~ variable, scales = "free") +
labs(title = "Numeric Distributions", x = "Value", y = "Frequency")
df_factor <- df_cls |>
select(where(is.factor)) |>
pivot_longer(cols = everything(), names_to = "variable", values_to = "value")
ggplot(df_factor, aes(x = value)) +
geom_bar() +
facet_wrap(~ variable, scales = "free") +
labs(title = "Factor Distributions", x = "Value", y = "Frequency")
train_index <- which(runif(nrow(df2)) < 0.8)
train_reg <- df_reg[train_index,]
test_reg <- df_reg[-train_index,]
train_cls <- df_cls[train_index,]
test_cls <- df_cls[-train_index,]
# Find Principal Components
pr <- prcomp(df_reg, scale=TRUE)
# Show Impact of Variables on the First Principle Component
pc1 <- pr$rotation[,1]
(pc1_loading_df <- cbind(names(pc1), as_data_frame(pc1)) |>
rename("names" = "names(pc1)", "pc1_loading" = "value") |>
arrange(desc(abs(pc1_loading))))
# Show Proportion of Variance Explained (PVE)
pve <- pr$sdev^2 / sum(pr$sdev^2)
pve_df <- as_data_frame(cbind(1:length(pve), pve)) %>% # Show PVE for each number of principal components
mutate(cumulative_pve = cumsum(pve)) %>% # Cumulative PVE for each additional PC
rename("num_principal_components" = "V1")
pve_df
# Show projections of data onto first 2 PCs
biplot(pr, scale = 0, cex=0.4)
km <- kmeans(pr$x, 3, nstart=20)
plot(pr$x, col = (km$cluster +1), pch=20, cex=2)
km <- kmeans(pr$x, 2, nstart=20)
plot(pr$x, col = (km$cluster +1), pch=20, cex=2)
km <- kmeans(pr$x, 4, nstart=20)
plot(pr$x, col = (km$cluster +1), pch=20, cex=2)
km <- kmeans(pr$x, 2, nstart=20)
plot(pr$x, col = (km$cluster +1), pch=20, cex=2)
km <- kmeans(pr$x, 2, nstart=50)
plot(pr$x, col = (km$cluster +1), pch=20, cex=2)
km <- kmeans(pr$x, 2, nstart=50)
plot(pr$x, col = (km$cluster +1), pch=20, cex=2)
km <- kmeans(pr$x, 2, nstart=50)
plot(pr$x, col = (km$cluster +1), pch=20, cex=2)
test_cls
train_reg
?runif
df_cls
df_reg
regsubsets(df_reg)
df_reg
regsubsets(gpa~., df_reg)
ncol(df_reg)
regsubsets(gpa~., nvmax = ncol(df_reg))
regsubsets(gpa~., nvmax = ncol(df_reg))
regsubsets(gpa~., df_reg, nvmax = ncol(df_reg))
best_subset <- regsubsets(gpa~., df_reg, nvmax = ncol(df_reg))
summary(best_subset)
summary(best_subset)
summary(best_subset)
best_subset <- regsubsets(gpa~., df_reg, nvmax = ncol(df_reg))
summary(best_subset)
summary(best_subset)
best_subset <- regsubsets(gpa~., df_reg, nvmax = ncol(df_reg))
best_subset_summary <- summary(best_subset)
best_subset <- regsubsets(gpa~., df_reg, nvmax = ncol(df_reg))
best_subset_summary <- summary(best_subset)
best_subset_summary
# Best subset selection
best_subset <- regsubsets(gpa~., df_reg, nvmax = ncol(df_reg))
bs_summary <- summary(best_subset)
plot(seq_len(10), bs_summary$bic)
# Best subset selection
best_subset <- regsubsets(gpa~., df_reg, nvmax = ncol(df_reg))
bs_summary <- summary(best_subset)
plot(seq_len(length(bs_summary$bic)), bs_summary$bic)
# Best subset selection
best_subset <- regsubsets(gpa~., df_reg, nvmax = ncol(df_reg))
bs_summary <- summary(best_subset)
plot(seq_len(length(bs_summary$bic)), bs_summary$bic, type="b")
# Best subset selection
best_subset <- regsubsets(gpa~., df_reg, nvmax = ncol(df_reg))
bs_summary <- summary(best_subset)
bs_min_bic <- which.min(bs_summary$bic)
plot(seq_len(length(bs_summary$bic)), bs_summary$bic, type="b")
points(bs_min_bic, min(bs_summary$bic), col="red", cex=2)
?points
# Best subset selection
best_subset <- regsubsets(gpa~., df_reg, nvmax = ncol(df_reg))
bs_summary <- summary(best_subset)
bs_min_bic <- which.min(bs_summary$bic)
plot(seq_len(length(bs_summary$bic)), bs_summary$bic, type="b")
points(bs_min_bic, min(bs_summary$bic), col="red", cex=2, pch=20)
# Best subset selection
best_subset <- regsubsets(gpa~., df_reg, nvmax = ncol(df_reg))
bs_summary <- summary(best_subset)
# BIC plot
plot(seq_len(length(bs_summary$bic)), bs_summary$bic, type="b")
points(which.min(bs_summary$bic), min(bs_summary$bic), col="red", cex=2, pch=20)
par(mfrow=(1,3))
par(mfrow=c(1,3))
# Best subset selection
best_subset <- regsubsets(gpa~., df_reg, nvmax = ncol(df_reg))
bs_summary <- summary(best_subset)
# BIC plot
plot(seq_len(length(bs_summary$bic)), bs_summary$bic, type="b")
points(which.min(bs_summary$bic), min(bs_summary$bic), col="red", cex=2, pch=20)
par(mfrow=c(1,3))
# Best subset selection
best_subset <- regsubsets(gpa~., df_reg, nvmax = ncol(df_reg))
bs_summary <- summary(best_subset)
# BIC plot
plot(seq_len(length(bs_summary$bic)), bs_summary$bic, type="b")
points(which.min(bs_summary$bic), min(bs_summary$bic), col="red", cex=2, pch=20)
# AIC plot
plot(seq_len(length(bs_summary$aic)), bs_summary$aic, type="b")
plot(seq_len(length(bs_summary$aic)), bs_summary$aic, type="b")
seq_len(length(bs_summary$aic))
bs_summary$aic
par(mfrow=c(1,3))
# Best subset selection
best_subset <- regsubsets(gpa~., df_reg, nvmax = ncol(df_reg))
bs_summary <- summary(best_subset)
# BIC plot
plot(seq_len(length(bs_summary$bic)), bs_summary$bic, type="b")
points(which.min(bs_summary$bic), min(bs_summary$bic), col="red", cex=2, pch=20)
# AIC plot
plot(seq_len(length(bs_summary$cp)), bs_summary$cp, type="b")
points(which.min(bs_summary$cp), min(bs_summary$cp), col="red", cex=2, pch=20)
par(mfrow=c(1,3))
# Best subset selection
best_subset <- regsubsets(gpa~., df_reg, nvmax = ncol(df_reg))
bs_summary <- summary(best_subset)
# BIC plot
plot(seq_len(length(bs_summary$bic)), bs_summary$bic, type="b")
points(which.min(bs_summary$bic), min(bs_summary$bic), col="red", cex=2, pch=20)
# cp plot
plot(seq_len(length(bs_summary$cp)), bs_summary$cp, type="b")
points(which.min(bs_summary$cp), min(bs_summary$cp), col="red", cex=2, pch=20)
# Adjusted R^2 plot
plot(seq_len(length(bs_summary$adjr2)), bs_summary$adjr2, type="b")
par(mfrow=c(1,3))
# Best subset selection
best_subset <- regsubsets(gpa~., df_reg, nvmax = ncol(df_reg))
bs_summary <- summary(best_subset)
# BIC plot
plot(seq_len(length(bs_summary$bic)), bs_summary$bic, type="b")
points(which.min(bs_summary$bic), min(bs_summary$bic), col="red", cex=2, pch=20)
# cp plot
plot(seq_len(length(bs_summary$cp)), bs_summary$cp, type="b")
points(which.min(bs_summary$cp), min(bs_summary$cp), col="red", cex=2, pch=20)
# Adjusted R^2 plot
plot(seq_len(length(bs_summary$adjr2)), bs_summary$adjr2, type="b")
points(which.max(bs_summary$adjr2), max(bs_summary$adjr2))
par(mfrow=c(1,3))
# Best subset selection
best_subset <- regsubsets(gpa~., df_reg, nvmax = ncol(df_reg))
bs_summary <- summary(best_subset)
# BIC plot
plot(seq_len(length(bs_summary$bic)), bs_summary$bic, type="b")
points(which.min(bs_summary$bic), min(bs_summary$bic), col="red", cex=2, pch=20)
# cp plot
plot(seq_len(length(bs_summary$cp)), bs_summary$cp, type="b")
points(which.min(bs_summary$cp), min(bs_summary$cp), col="red", cex=2, pch=20)
# Adjusted R^2 plot
plot(seq_len(length(bs_summary$adjr2)), bs_summary$adjr2, type="b")
points(which.max(bs_summary$adjr2), max(bs_summary$adjr2), col="red", cex=2, pch=20)
par(mfrow=c(12,3))
# Best subset selection
best_subset <- regsubsets(gpa~., df_reg, nvmax = ncol(df_reg))
bs_summary <- summary(best_subset)
# BIC plot
plot(seq_len(length(bs_summary$bic)), bs_summary$bic, type="b")
par(mfrow=c(1,3))
# Best subset selection
best_subset <- regsubsets(gpa~., df_reg, nvmax = ncol(df_reg))
bs_summary <- summary(best_subset)
# BIC plot
plot(seq_len(length(bs_summary$bic)), bs_summary$bic, type="b")
points(which.min(bs_summary$bic), min(bs_summary$bic), col="red", cex=2, pch=20)
# cp plot
plot(seq_len(length(bs_summary$cp)), bs_summary$cp, type="b")
points(which.min(bs_summary$cp), min(bs_summary$cp), col="red", cex=2, pch=20)
# Adjusted R^2 plot
plot(seq_len(length(bs_summary$adjr2)), bs_summary$adjr2, type="b")
points(which.max(bs_summary$adjr2), max(bs_summary$adjr2), col="red", cex=2, pch=20)
par(mfrow=c(3,1))
# Best subset selection
best_subset <- regsubsets(gpa~., df_reg, nvmax = ncol(df_reg))
bs_summary <- summary(best_subset)
# BIC plot
plot(seq_len(length(bs_summary$bic)), bs_summary$bic, type="b")
points(which.min(bs_summary$bic), min(bs_summary$bic), col="red", cex=2, pch=20)
# cp plot
plot(seq_len(length(bs_summary$cp)), bs_summary$cp, type="b")
points(which.min(bs_summary$cp), min(bs_summary$cp), col="red", cex=2, pch=20)
# Adjusted R^2 plot
plot(seq_len(length(bs_summary$adjr2)), bs_summary$adjr2, type="b")
points(which.max(bs_summary$adjr2), max(bs_summary$adjr2), col="red", cex=2, pch=20)
par(mfrow=c(1,3))
# Best subset selection
best_subset <- regsubsets(gpa~., df_reg, nvmax = ncol(df_reg))
bs_summary <- summary(best_subset)
# BIC plot
plot(seq_len(length(bs_summary$bic)), bs_summary$bic, type="b")
points(which.min(bs_summary$bic), min(bs_summary$bic), col="red", cex=2, pch=20)
# cp plot
plot(seq_len(length(bs_summary$cp)), bs_summary$cp, type="b")
points(which.min(bs_summary$cp), min(bs_summary$cp), col="red", cex=2, pch=20)
# Adjusted R^2 plot
plot(seq_len(length(bs_summary$adjr2)), bs_summary$adjr2, type="b")
points(which.max(bs_summary$adjr2), max(bs_summary$adjr2), col="red", cex=2, pch=20)
par(mfrow=c(1,3))
# Best subset selection
best_subset <- regsubsets(gpa~., df_reg, nvmax = ncol(df_reg))
bs_summary <- summary(best_subset)
# BIC plot
plot(seq_len(length(bs_summary$bic)), bs_summary$bic, type="b", xlab="Number of predictors", ylab="Score")
points(which.min(bs_summary$bic), min(bs_summary$bic), col="red", cex=2, pch=20)
# cp plot
plot(seq_len(length(bs_summary$cp)), bs_summary$cp, type="b", xlab="Number of predictors", ylab="Score")
points(which.min(bs_summary$cp), min(bs_summary$cp), col="red", cex=2, pch=20)
# Adjusted R^2 plot
plot(seq_len(length(bs_summary$adjr2)), bs_summary$adjr2, type="b", xlab="Number of predictors", ylab="Score")
points(which.max(bs_summary$adjr2), max(bs_summary$adjr2), col="red", cex=2, pch=20)
git add .
# Show projections of data onto first 2 PCs
biplot(pr, scale = 0, cex=0.4)

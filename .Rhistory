) |>
filter(max_cat == "TS" | max_cat == "TD") |>
group_by(Basin, NameYear) |>
summarize(
season_begin = min(ymd_extract(as.numeric(NameYear), observ_time), na.rm = TRUE),
season_end = max(ymd_extract(as.numeric(NameYear), observ_time), na.rm = TRUE),
) |>
ungroup()
)
(seasons <- cyclones |>
summarize(
.by = c("Basin", "NameYear", "Number"),
max_cat = max(category, na.rm = TRUE),
.groups = "drop"
) |>
filter(max_cat == "TS" | max_cat == "TD") |>
summarize(
.by = c("Basin", "NameYear"),
season_begin = min(ymd_extract(as.numeric(NameYear), observ_time), na.rm = TRUE),
season_end = max(ymd_extract(as.numeric(NameYear), observ_time), na.rm = TRUE),
) |>
ungroup()
)
(seasons <- cyclones |>
summarize(
.by = c("Basin", "NameYear", "Number"),
max_cat = max(category, na.rm = TRUE),
.groups = "drop"
) |>
filter(max_cat == "TS" | max_cat == "TD") |>
summarize(
.by = c("Basin", "NameYear"),
season_begin = min(ymd_extract(as.numeric(NameYear), observ_time), na.rm = TRUE),
season_end = max(ymd_extract(as.numeric(NameYear), observ_time), na.rm = TRUE)
)
)
(seasons <- cyclones |>
summarize(
.by = c("Basin", "NameYear", "Number"),
max_cat = max(category, na.rm = TRUE),
) |>
filter(max_cat == "TS" | max_cat == "TD") |>
summarize(
.by = c("Basin", "NameYear"),
season_begin = min(ymd_extract(as.numeric(NameYear), observ_time), na.rm = TRUE),
season_end = max(ymd_extract(as.numeric(NameYear), observ_time), na.rm = TRUE)
)
)
(seasons <- cyclones |>
group_by(Basin,NameYear,Number) |>
summarize(
max_cat = max(category, na.rm = TRUE),
) |>
ungroup() |>
filter(max_cat == "TS" | max_cat == "TD") |>
summarize(
.by = c("Basin", "NameYear"),
season_begin = min(ymd_extract(as.numeric(NameYear), observ_time), na.rm = TRUE),
season_end = max(ymd_extract(as.numeric(NameYear), observ_time), na.rm = TRUE)
)
)
(seasons <- cyclones |>
group_by(Basin,NameYear,Number) |>
mutate(
max_cat = max(category, na.rm = TRUE),
) |>
ungroup() |>
filter(max_cat == "TS" | max_cat == "TD") |>
summarize(
.by = c("Basin", "NameYear"),
season_begin = min(ymd_extract(as.numeric(NameYear), observ_time), na.rm = TRUE),
season_end = max(ymd_extract(as.numeric(NameYear), observ_time), na.rm = TRUE)
)
)
(seasons_segment_plot <- seasons |>
ggplot(aes(x=season_begin,y=NameYear)) +
geom_point() +
geom_segment(aes(xend=season_end,yend=NameYear)) +
labs(
title = "Storms that grow to hurricanes are seen during longer seasons in a year",
subtitle =  "The seasons in Atlantic looks more varied than the Pacific",
caption =  "Data from NOAA",
x = "Duration of the hurricane seasons",
y = "Year"
) + facet_wrap(~Basin)
)
(seasons_length_plot  <- seasons
|> mutate(
length_days = interval(season_begin, season_end)/day(1)
) |>
ggplot(aes(x=NameYear,y=length_days,color=Basin)) +
geom_line() +
geom_smooth(aes(color="orange"))
)
(seasons_length_plot  <- seasons
|> mutate(
length_days = interval(season_begin, season_end)/day(1)
) |>
ggplot(aes(x=NameYear,y=length_days,color=Basin)) +
geom_line() +
geom_smooth(aes(color="orange"))
) + labs(
title = "Storms that grow to hurricanes are seen during longer seasons in a year",
subtitle = "The growing trend looks more steady in the Atlantic basin",
caption = "Data from NOAA",
x = "Year",
y = "Length of the hurricane seasons"
)
(seasons_length_plot  <- seasons
|> mutate(
length_days = interval(season_begin, season_end)/day(1)
) |>
ggplot(aes(x=NameYear,y=length_days,color=Basin)) +
geom_line() +
geom_smooth(color="orange")
) + labs(
title = "Storms that grow to hurricanes are seen during longer seasons in a year",
subtitle = "The growing trend looks more steady in the Atlantic basin",
caption = "Data from NOAA",
x = "Year",
y = "Length of the hurricane seasons"
)
(seasons <- cyclones |>
group_by(Basin,NameYear,Number) |>
mutate(
max_cat = max(category, na.rm = TRUE),
) |>
ungroup() |>
filter(max_cat != "TS" & max_cat != "TD") |>
summarize(
.by = c("NameYear","Basin"),
season_begin = min(ymd_extract(as.numeric(NameYear), observ_time), na.rm = TRUE),
season_end = max(ymd_extract(as.numeric(NameYear), observ_time), na.rm = TRUE)
)
)
(seasons_segment_plot <- seasons |>
ggplot(aes(x=season_begin,y=NameYear)) +
geom_point() +
geom_segment(aes(xend=season_end,yend=NameYear)) +
labs(
title = "Storms that grow to hurricanes are seen during longer seasons in a year",
subtitle =  "The seasons in Atlantic looks more varied than the Pacific",
caption =  "Data from NOAA",
x = "Duration of the hurricane seasons",
y = "Year"
) + facet_wrap(~Basin)
)
(seasons_length_plot  <- seasons
|> mutate(
length_days = interval(season_begin, season_end)/day(1)
) |>
ggplot(aes(x=NameYear,y=length_days,color=Basin)) +
geom_line() +
geom_smooth(color="orange")
) + labs(
title = "Storms that grow to hurricanes are seen during longer seasons in a year",
subtitle = "The growing trend looks more steady in the Atlantic basin",
caption = "Data from NOAA",
x = "Year",
y = "Length of the hurricane seasons"
)
(seasons_length_plot  <- seasons
|> mutate(
length_days = interval(season_begin, season_end)/day(1)
) |>
ggplot(aes(x=NameYear,y=length_days)) +
geom_line(aes(color=Basin)) +
geom_smooth(color="orange")
) + labs(
title = "Storms that grow to hurricanes are seen during longer seasons in a year",
subtitle = "The growing trend looks more steady in the Atlantic basin",
caption = "Data from NOAA",
x = "Year",
y = "Length of the hurricane seasons"
)
#NOTES TO SELF:::::::::
#-GPA is sum of 4 Classes, So any model cant use all 4
knitr::opts_chunk$set(echo = TRUE)
if(! require("caret")){install.packages("caret")}
if(! require("tidyverse")){install.packages("tidyverse")}
if(! require("ISLR2")){install.packages("ISLR2")}
if(! require("boot")){install.packages("boot")}
if(! require("MASS") ){ install.packages("MASS") }
if(! require("leaps") ){ install.packages("leaps") }
if(! require("glmnet") ){ install.packages("glmnet") }
if(! require("pls") ){ install.packages("pls") }
if(! require("splines")) {install.packages("splines")}
if(! require("e1071")){ install.packages("e1071")}
if(! require("pROC")){ install.packages("pROC")}
if(! require("class")){ install.packages("class")}
if(! require("reshape2")){ install.packages("reshape2")}
if(! require("DAAG")){ install.packages("DAAG")}
if(! require("ROCR")){ install.packages("ROCR")}
if(! require("tree")){ install.packages("tree")}
if(! require("randomForest")){ install.packages("randomForest")}
library(tidyverse)
library(e1071)
library(MASS)
library(tidyverse)
library(caret)
library(leaps)
library(tidyr)
library(ggplot2)
library(glmnet)
library(ROCR)
library(pROC)
library(randomForest)
library(tree)
select <- dplyr::select
contents <- tribble(
~Index, ~Category, ~Method,
1, "Introduction", "Written",
2, "Preprocessing", "Data Description and Encoding",
3, "Unsupervised Learning/Analysis", "PCA and Clustering",
4, "Preprocessing", "Variable Selection",
5, "Supervised Regression", "Linear and Non-Linear Regression",
6, "Supervised Classification", "LDA/QDA and Logistic Regression",
7, "Supervised Classification", "Tree-Based Methods",
8, "Supervised Classification", "Support Vector Machines",
9, "Deep Learning", "Neural Networks",
10, "Conclusion", "Written",
)
contents
location = "https://raw.githubusercontent.com/BrandonYChan/Statistical-Learning/main/Student_performance_10k.csv"
(df <- read_csv(location, show_col_types = FALSE, n_max=10000))
df2 <- df |>
rename(
"has_subsidized_lunch" = "lunch",
"has_prepared" = "test_preparation_course",
"is_male" = "gender",
"parent_education" = "parental_level_of_education",
"gpa" = "total_score",
"gpa_letter" = "grade"
) |>
mutate(
is_male = as.integer(if_else(is_male =="male",1,0)),
race_ethnicity = as.factor(str_sub(df$race_ethnicity, -1, -1)),
parent_education = as.factor(parent_education),
gpa = gpa/4,
gpa_letter = as.factor(if_else(gpa_letter == "Fail", "F", gpa_letter))
) |>
select(-c("roll_no","science_score","writing_score"))
#Remove Any Duplicates
df3 <- distinct(df2)
#Replace Missing NUMERIC Values With Column Mean
if (any(is.na(df3))) {
df3[] <- lapply(df3, function(x) {
if (is.numeric(x)) {
replace(x, is.na(x), mean(x, na.rm = TRUE))
} else {
x
}
})
}
#Dropping Missing Factor Columns
df3 <- df3[complete.cases(df3[c("race_ethnicity", "parent_education", "gpa_letter")]), ]
#Display Numeric
df_numeric <- df3 |> select(where(is.numeric)) |> pivot_longer(cols = everything(), names_to = "variable", values_to = "value")
ggplot(df_numeric, aes(x = value)) + geom_histogram(bins = 30) +  facet_wrap(~ variable, scales = "free")
#Display Factor
df_factor <- df3 |> select(where(is.factor)) |> pivot_longer(cols = everything(), names_to = "variable", values_to = "value")
ggplot(df_factor, aes(x = value)) + geom_bar() + facet_wrap(~ variable, scales = "free")
df4 <- df3 |> bind_cols(as.data.frame(model.matrix(~ `race_ethnicity` + `parent_education` - 1, data = df3))) |>
select(-c("race_ethnicity","parent_education"))
set.seed(10)
#Splits Rows
train_index <- sample(1:nrow(df4), nrow(df4)*0.8)
df_train <- df4[train_index,]
df_test <- df4[-train_index,]
# Find Principal Components
pc <- prcomp(df_train |> select(-c("gpa_letter")), scale=TRUE)
pc2 <- prcomp(df_test |> select(-c("gpa_letter")), scale=TRUE)
#Highlight Distribution Of TRAIN GPA in PC
gpa_colors <- as.factor(df_train$gpa_letter)
plot(pc$x, col = gpa_colors, pch=20, cex=2)
legend("topright", legend = levels(gpa_colors),
fill = 1:length(levels(gpa_colors)),
pch = 20, title = "GPA Letter")
# Not supposed to look at the test set
#Highlight Distribution of TEST GPA in PC
# gpa_colors <- as.factor(df_test$gpa_letter)
# plot(pc2$x, col = gpa_colors, pch=20, cex=2)
# legend("topright", legend = levels(gpa_colors),
#        fill = 1:length(levels(gpa_colors)),
#        pch = 20, title = "GPA Letter")
pc1 <- pc$rotation[,1]
(pc1_loading_df <- cbind(names(pc1), as_data_frame(pc1)) |>
rename("names" = "names(pc1)", "pc1_loading" = "value") |>
arrange(desc(abs(pc1_loading))))
pve <- pc$sdev^2 / sum(pc$sdev^2)
(pve_df <- as_data_frame(cbind(1:length(pve), pve)) %>% # Show PVE for each number of principal components
mutate(cumulative_pve = cumsum(pve)) %>% # Cumulative PVE for each additional PC
rename("num_principal_components" = "V1"))
biplot(pc, scale = 0, cex=0.4)
km <- kmeans(pc$x, 6, nstart=50)
plot(pc$x, col = (km$cluster +1), pch=20, cex=2)
par(mfrow=c(1,3))
# Best subset selection
best_subset <- regsubsets(gpa~., df_train |> select(-c("gpa_letter")), nvmax = ncol(df_train))
(bs_summary <- summary(best_subset))
# BIC plot
plot(seq_len(length(bs_summary$bic)), bs_summary$bic, type="b", xlab="Number of predictors", ylab="BIC")
points(which.min(bs_summary$bic), min(bs_summary$bic), col="red", cex=2, pch=20)
# cp plot
plot(seq_len(length(bs_summary$cp)), bs_summary$cp, type="b", xlab="Number of predictors", ylab="CP",ylim=c(0,20))
points(which.min(bs_summary$cp), min(bs_summary$cp), col="red", cex=2, pch=20)
# lines(seq_along(bs_summary$cp), seq_along(bs_summary$cp)+1, col="gray", lty=2)
# Adjusted R^2 plot
plot(seq_len(length(bs_summary$adjr2)), bs_summary$adjr2, type="b", xlab="Number of predictors", ylab="R^2",ylim=c(0.45,1))
points(which.max(bs_summary$adjr2), max(bs_summary$adjr2), col="red", cex=2, pch=20)
tibble(Best_BIC = which.min(bs_summary$bic), Best_Cp = which.min(bs_summary$cp), Best_Adjusted_Rsq = which.max(bs_summary$adjr2))
#BIC HEAVILY PENALIZED COMPLEXITY, BUT ADRSQ, AND CP BOTH SEEM TO BE OKAY WITH HIGHER, AND THE FACT LOTS OF THESE ARE ONE HOT VARIABLES MAKES ME THINK SLIGHTLY MORE IS OKAY, MAYBE WE JUST CHOOISE SOMETHING RANDOM FOR NOW, USE IT TO MOVE ON, THEN COME BACK
p = 6
chosen_model <- bs_summary$which[p, ]
chosen_predictors = gsub("`", "", names(chosen_model[chosen_model == TRUE]))
chosen_predictors
#UPDATING TRAIN + TEST TO ONLY INCLUDE IMPORTANT VARIABLES
df_train <- df_train[, c("gpa","gpa_letter", chosen_predictors[2:length(chosen_predictors)])]
df_test <- df_test[, c("gpa","gpa_letter", chosen_predictors[2:length(chosen_predictors)])]
df_test
#Generating Matrix Form
train_x <- as.matrix(df_train[, -c(1,2)])
train_y <- as.matrix(df_train$gpa)
test_x <- as.matrix(df_test[, -c(1,2)])
test_y <- as.matrix(df_test$gpa)
#Linear Model
linear_model <- lm(gpa ~ ., data=df_train[,-c(2)])
linear_preds <- predict(linear_model, df_test[,-c(2)])
RMSE_linear = RMSE(test_y, linear_preds)
#LASSO:
optimal_lambda <- cv.glmnet(train_x, train_y, alpha=1)$lambda.min
lasso_model <- glmnet(train_x, train_y, alpha=1, lambda=optimal_lambda)
lasso_predictions <- predict(lasso_model, newx=test_x, s=optimal_lambda)
RMSE_lasso =  RMSE(test_y, lasso_predictions)
#Ridge
optimal_lambda <- cv.glmnet(train_x, train_y, alpha=0)$lambda.min
ridge_model <- glmnet(train_x, train_y, alpha=0, lambda=optimal_lambda)
ridge_predictions <- predict(ridge_model, newx=test_x, s=optimal_lambda)
RMSE_ridge = RMSE(test_y, ridge_predictions)
tribble(~"Unregularised Linear RMSE", ~"LASSO RMSE", ~"Ridge RMSE",
RMSE_linear, RMSE_lasso, RMSE_ridge)
# LASSO regression performs the best of these
#THEREFORE OUR BEST MODEL (AND INSIGHT FROM COEFFICIENTS COMES FROM LASSO)
coef(lasso_model)
#LASSO MODEL EVALUATION:
error_colors <- as.factor(abs(df_test$gpa - lasso_predictions) < 10)
plot(pc2$x, col = error_colors, pch=20, cex=2)
legend("topright", legend = levels(error_colors),
fill = 1:length(levels(error_colors)),
pch = 20, title = "Reasonable Prediction")
# Polynomial Regression
# Natural Splines
# Generalized Additive Model (GAM)
lda_model <- lda(gpa_letter ~ ., data = df_train |> select(-c("gpa")))
lda_preds <- predict(lda_model, newdata = df_test |> select(-c("gpa")))
lda_class <- lda_preds$class
table(lda_class, df_test$gpa_letter)
accuracy <- mean(lda_class == df_test$gpa_letter)
print(accuracy)
summary(lda_model)
#LDA MODEL EVALUATION:
error_colors <- as.factor(df_test$gpa_letter == lda_class)
plot(pc2$x, col = error_colors, pch=20, cex=2)
legend("topright", legend = levels(error_colors),
fill = 1:length(levels(error_colors)),
pch = 20, title = "Reasonable Prediction")
predicted_probs <- lda_preds$posterior
colnames(predicted_probs) <- levels(df_test$gpa_letter)
#QDA:
#READ:
#some groups are too small size for QDA to work (its an actual error), so as a group we gotta decide how we want to handle that
# I think we just get rid of it and group LDA with logistic regression under the same header
#LOGISTIC REGRESSION
library(nnet)
log_reg_model <- multinom(gpa_letter ~ ., data = df_train |> select(-c("gpa")))
#log_reg_pred_probs <- predict(log_reg_model, newdata = test_cls, type = "probs")
log_reg_pred_class <- predict(log_reg_model, newdata = df_test |> select(-c("gpa")), type = "class")
log_reg_accuracy <- mean(log_reg_pred_class == df_test$gpa_letter)
#LOGISTIC MODEL EVALUATION:
error_colors <- as.factor(log_reg_pred_class == df_test$gpa_letter)
plot(pc2$x, col = error_colors, pch=20, cex=2)
legend("topright", legend = levels(error_colors),
fill = 1:length(levels(error_colors)),
pch = 20, title = "Reasonable Prediction")
table(log_reg_pred_class, df_test$gpa_letter)
print(log_reg_accuracy)
# summary(log_reg_model)
library(tree)
train_input <- df_train %>% select(-c(gpa)) %>% rename(masters_degree="parent_educationmaster's degree") # This variable is messed up
decision_tree <- tree(gpa_letter ~ ., train_input)
summary(decision_tree)
plot(decision_tree)
text(decision_tree)
random_forest_model <- randomForest(gpa_letter~., train_input, ntree=500, importance=TRUE)
summary(random_forest_model)
random_forest_model$confusion
importance(random_forest_model)
varImpPlot(random_forest_model)
# # This won't fully run on my laptop (been going 10 mins) so it won't knit
#
#IF YOU LAPTOP IS STRUGGLING WE DONT HAVE TO USE BEST WE CAN JUST USE SOME DEFAULT IE: 1
# (SEE BELOW)
# #SVM
library(e1071)
#
try = seq(1,1 #10
,by=1)
#
# #ATTEMPTING TO DETERMINE THE RELATIONSHIP OF DATA (IE LINEAR,QUADRATIC,RADIAL)
#
# #from PC, we expect Linear Model To Work Well
#
#
train <- df_train |> select(-c("gpa"))
test <- df_test |> select(-c("gpa"))
#
best <- tune(svm, gpa_letter ~ ., data = train, ranges = list(cost = try),kernel="linear")$best.parameters$cost
print(best)
linear_kernel_svm <- svm(gpa_letter ~ .,data=train, cost=best, kernel="linear")
lin_preds <- predict(linear_kernel_svm, test, type="class") != test$gpa_letter
lin_mis <- mean(lin_preds)
best <- tune(svm, gpa_letter ~ ., data = train, ranges = list(cost = try), kernel="radial")$best.parameters$cost
print(best)
radial_kernel_svm <- svm(gpa_letter ~ .,data=train, cost=best, kernel="radial")
rad_mis <- mean(predict(radial_kernel_svm, test, type="class") != test$gpa_letter)
best <- tune(svm, gpa_letter ~ ., data = train, ranges = list(cost = try), kernel="polynomial",degree=2)$best.parameters$cost
print(best)
quadratic_kernel_svm <- svm(gpa_letter ~ .,data=train, cost=best, kernel="polynomial",degree=2)
quad_mis <- mean(predict(quadratic_kernel_svm, test, type="class") != test$gpa_letter)
#
tribble(~"LINEAR MIS", ~"RADIAL MIS", ~"QUAD MIS",
lin_mis, rad_mis, quad_mis)
error_colors <- as.factor(!lin_preds)
plot(pc2$x, col = error_colors, pch=20, cex=2)
legend("topright", legend = levels(error_colors),
fill = 1:length(levels(error_colors)),
pch = 20, title = "Reasonable Prediction")
# Find Principal Components
pc <- prcomp(df_train |> select(-c("gpa_letter")), scale=TRUE)
pc2 <- prcomp(df_test |> select(-c("gpa_letter")), scale=TRUE)
#Highlight Distribution Of TRAIN GPA in PC
gpa_colors <- as.factor(df_train$gpa_letter)
plot(pc$x, col = gpa_colors, pch=20, cex=2)
legend("topright", legend = levels(gpa_colors),
fill = 1:length(levels(gpa_colors)),
pch = 20, title = "GPA Letter")
# Not supposed to look at the test set
#Highlight Distribution of TEST GPA in PC
# gpa_colors <- as.factor(df_test$gpa_letter)
# plot(pc2$x, col = gpa_colors, pch=20, cex=2)
# legend("topright", legend = levels(gpa_colors),
#        fill = 1:length(levels(gpa_colors)),
#        pch = 20, title = "GPA Letter")
pve <- pc$sdev^2 / sum(pc$sdev^2)
(pve_df <- as_data_frame(cbind(1:length(pve), pve)) %>% # Show PVE for each number of principal components
mutate(cumulative_pve = cumsum(pve)) %>% # Cumulative PVE for each additional PC
rename("num_principal_components" = "V1"))
biplot(pc, scale = 0, cex=0.4)
par(mfrow=c(1,3))
# Best subset selection
best_subset <- regsubsets(gpa~., df_train |> select(-c("gpa_letter")), nvmax = ncol(df_train))
(bs_summary <- summary(best_subset))
# BIC plot
plot(seq_len(length(bs_summary$bic)), bs_summary$bic, type="b", xlab="Number of predictors", ylab="BIC")
points(which.min(bs_summary$bic), min(bs_summary$bic), col="red", cex=2, pch=20)
# cp plot
plot(seq_len(length(bs_summary$cp)), bs_summary$cp, type="b", xlab="Number of predictors", ylab="CP",ylim=c(0,20))
points(which.min(bs_summary$cp), min(bs_summary$cp), col="red", cex=2, pch=20)
# lines(seq_along(bs_summary$cp), seq_along(bs_summary$cp)+1, col="gray", lty=2)
# Adjusted R^2 plot
plot(seq_len(length(bs_summary$adjr2)), bs_summary$adjr2, type="b", xlab="Number of predictors", ylab="R^2",ylim=c(0.45,1))
points(which.max(bs_summary$adjr2), max(bs_summary$adjr2), col="red", cex=2, pch=20)
#LOGISTIC REGRESSION
library(nnet)
log_reg_model <- multinom(gpa_letter ~ ., data = df_train |> select(-c("gpa")))
#log_reg_pred_probs <- predict(log_reg_model, newdata = test_cls, type = "probs")
log_reg_pred_class <- predict(log_reg_model, newdata = df_test |> select(-c("gpa")), type = "class")
log_reg_accuracy <- mean(log_reg_pred_class == df_test$gpa_letter)
#LOGISTIC MODEL EVALUATION:
error_colors <- as.factor(log_reg_pred_class == df_test$gpa_letter)
plot(pc2$x, col = error_colors, pch=20, cex=2)
legend("topright", legend = levels(error_colors),
fill = 1:length(levels(error_colors)),
pch = 20, title = "Reasonable Prediction")
table(log_reg_pred_class, df_test$gpa_letter)
#LOGISTIC MODEL EVALUATION:
error_colors <- as.factor(log_reg_pred_class == df_test$gpa_letter)
plot(pc2$x, col = error_colors, pch=20, cex=2)
legend("topright", legend = levels(error_colors),
fill = 1:length(levels(error_colors)),
pch = 20, title = "Reasonable Prediction")
print(mean(log_reg_pred_class == df_test$gpa_letter))
table(log_reg_pred_class, df_test$gpa_letter)
library(tree)
train_input <- df_train %>% select(-c(gpa)) %>% rename(masters_degree="parent_educationmaster's degree") # This variable is messed up
decision_tree <- tree(gpa_letter ~ ., train_input)
summary(decision_tree)
plot(decision_tree)
text(decision_tree)
library(tree)
train_input <- df_train %>% select(-c(gpa)) %>% rename(masters_degree="parent_educationmaster's degree") # This variable is messed up
decision_tree <- tree(gpa_letter ~ ., train_input)
summary(decision_tree)
plot(decision_tree)
text(decision_tree)
library(tree)
train_input <- df_train %>% select(-c(gpa)) %>% rename(masters_degree="parent_educationmaster's degree") # This variable is messed up
decision_tree <- tree(gpa_letter ~ ., train_input)
summary(decision_tree)
plot(decision_tree)
text(decision_tree)
?tree

---
title: "Predicting and Understanding NBA Career Length"
author: "Brandon Chan + Lucas Duncan"
date: "2025-03-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

if(! require("caret")){install.packages("caret")}
if(! require("tidyverse")){install.packages("tidyverse")}
if(! require("ISLR2")){install.packages("ISLR2")}
if(! require("boot")){install.packages("boot")}
if(! require("MASS") ){ install.packages("MASS") }
if(! require("leaps") ){ install.packages("leaps") }
if(! require("glmnet") ){ install.packages("glmnet") }
if(! require("pls") ){ install.packages("pls") }
if(! require("splines")) {install.packages("splines")}
if(! require("e1071")){ install.packages("e1071")}
if(! require("pROC")){ install.packages("pROC")}
if(! require("class")){ install.packages("class")}
if(! require("reshape2")){ install.packages("reshape2")}
if(! require("DAAG")){ install.packages("DAAG")}
if(! require("pROC")){ install.packages("pROC")}
if(! require("ROCR")){ install.packages("ROCR")}

library(tidyverse)
library(e1071)
library(MASS)
library(tidyverse)
library(caret)
library(leaps)
library(pROC)
library(ROCR)

select <- dplyr::select
```

## Executive Summary and Contents

```{r}
contents <- tribble(
  ~Index, ~Category, ~Method,
  1, "Introduction", "Written",
  2, "Preprocessing", "Data Description and Encoding",
  3, "Unsupervised Learning/Analysis", "PCA and Clustering",
  4, "Preprocessing", "Variable Selection",
  5, "Supervised Regression", "Linear and Non-Linear Regression",
  6, "Supervised Classification", "LDA/QDA and Logistic Regression",
  7, "Supervised Classification", "Tree-Based Methods",
  8, "Supervised Classification", "Support Vector Machines",
  9, "Deep Learning", "Neural Networks",
  10, "Conclusion", "Written", 
)
contents

```


# 1 Preprocessing

#1.1 Extract Data
```{r, include=FALSE}
# Load data from GitHub and preprocess 
df <- read_csv("https://raw.githubusercontent.com/BrandonYChan/Statistical-Learning/main/basketball_career_length.csv")


```

#1.2 Mutate Dataframe
```{r, echo=FALSE}

format="%B %d, %Y"

df2 <- df |>
  
       rename(
         #Renaming Ambiguous Status Variable
         "retired" = "status"     
       ) |>
       mutate(
         #Convert BirthDate To Year
         birth_date = as.Date(birth_date,format),  
         
         #Store Age At Start
         age = start_year - year(birth_date),  
         
         #Convert Retired,HallOfFame To Integer
         retired = as.integer(retired),                         
         hall_of_fame = as.integer(hall_of_fame)
       ) |>
  
       #Removing Redundant And Non Numeric
       select(-c(
                 name,   #Not Important To Model
                 sport,  #All Identical
                 end_year, #Redundant As Start,Duration Already Stored
                 birth_date  #Redundant As Start,Age Already Stored
                 ))




```
#1.3 Filter Data + Handle Missing
```{r}
#Remove Duplicate Rows
df3 <- distinct(df2)

#Replace Missing NUMERIC (Including Date) Values With ColMean
if (any(is.na(df3))) {
  df3[] <- lapply(df3, function(x) {
    if (is.numeric(x)) {
      replace(x, is.na(x), mean(x, na.rm = TRUE))
    } else {
      x
    }
  })
}
```

#1.4
```{r}
df_reg <- df3 |> bind_cols(as.data.frame(model.matrix(~ positions - 1, data = df3))) |>
               select(-c(positions))
  
  
df_cls <- df3


```

#1.4 Display Final Format
```{r}
df_reg[sample(nrow(df_reg), size = 10), ]
```

#1.5 Predictor Distribution:
```{r}

data_long <- pivot_longer(df_reg, cols = everything())

ggplot(data_long, aes(x = value, fill = name)) +
  geom_histogram(bins = 20, position = "identity") +
  facet_wrap(~ name, scales = "free") +
  labs(title = "Predictor Distributions", x = "Value", y = "Frequency")
```

#1.6 Genereting Data For Regression + Classification
```{r}

#Generating One Hot Representation For Non Numeric


```




# 2 Data Exploration with Unsupervised Methods 


##2.1 Principal Component Analysis

```{r}

# Find Principal Components
pr <- prcomp(df3, scale=TRUE)

pc1 <- pr$rotation[,1]

# Show Impact of Variables on the First Principle Component
pc1_loading_df <- cbind(names(pc1), as_data_frame(pc1)) %>% # Join names and loadings to form columns in the dataframe
  rename("names" = "names(pc1)", "pc1_loading" = "value") %>%  
  arrange(desc(abs(pc1_loading)))

pc1_loading_df
```


```{r}
# Show Proportion of Variance Explained (PVE)
pve <- pr$sdev^2 / sum(pr$sdev^2)   

pve_df <- as_data_frame(cbind(1:length(pve), pve)) %>% # Show PVE for each number of principal components
  mutate(cumulative_pve = cumsum(pve)) %>% # Cumulative PVE for each additional PC 
  rename("num_principal_components" = "V1")
  
pve_df
```


```{r}
# Show projections of data onto first 2 PCs
biplot(pr, scale = 0, cex=0.4)
```

## K-Means Clustering on PC1 and PC2

```{r}
km <- kmeans(pr$x, 3, nstart=20)
plot(pr$x, col = (km$cluster +1), pch=20, cex=2)
```

# Variable Selection 
```{r}

```

## Train Test Split
```{r}
train_index = sample(seq_len(nrow(df3)), 0.8*nrow(df3))
train <- df3[train_index,]
test <- df3[-train_index,]

nrow(train)
nrow(test)
```






# Tree
```{r}
library(tree)

#Generate Trees of Various size

#Display Error Rate At Given Sizes (Train, Test)

general_model <- tree(position ~ ., data = train)

general_predictions <- predict(general_model, test, type = "class")


general_cv = cv.tree(general_model, FUN = prune.misclass)


plot(general_cv$size, general_cv$dev, type = "b", 
     xlab = " Size", 
     ylab = "Misclassification",
     main = " Error vs. Size")



#Choose Best

#Generate Best Tree

#Comment On Accuracy



```







---
title: "Predicting and Understanding NBA Career Length"
author: "Brandon Chan + Lucas Duncan"
date: "2025-03-07"
output: html_document
---

```{r}
#NOTES TO SELF:::::::::
  
#-GPA is sum of 4 Classes, So any model cant use all 4

# Additionally we could make the classification stuff boolean (desirable or not)
#   SEARCH FOR "CLS-FIX"

```

## 2.0: Imports
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

if(! require("caret")){install.packages("caret")}
if(! require("tidyverse")){install.packages("tidyverse")}
if(! require("ISLR2")){install.packages("ISLR2")}
if(! require("boot")){install.packages("boot")}
if(! require("MASS") ){ install.packages("MASS") }
if(! require("leaps") ){ install.packages("leaps") }
if(! require("glmnet") ){ install.packages("glmnet") }
if(! require("pls") ){ install.packages("pls") }
if(! require("splines")) {install.packages("splines")}
if(! require("e1071")){ install.packages("e1071")}
if(! require("pROC")){ install.packages("pROC")}
if(! require("class")){ install.packages("class")}
if(! require("reshape2")){ install.packages("reshape2")}
if(! require("DAAG")){ install.packages("DAAG")}
if(! require("ROCR")){ install.packages("ROCR")}
if(! require("tree")){ install.packages("tree")}
if(! require("randomForest")){ install.packages("randomForest")}

library(tidyverse)
library(e1071)
library(MASS)
library(tidyverse)
library(caret)
library(leaps)
library(tidyr)
library(ggplot2)
library(cluster)
library(glmnet)
library(ROCR)
library(pROC)
library(randomForest)
library(tree)
select <- dplyr::select
```


## Executive Summary and Contents
```{r}
contents <- tribble(
  ~Index, ~Category, ~Method,
  1, "Introduction", "Written",
  2, "Preprocessing", "Data Description and Encoding",
  3, "Unsupervised Learning/Analysis", "PCA and Clustering",
  4, "Preprocessing", "Variable Selection",
  5, "Supervised Regression", "Linear and Non-Linear Regression",
  6, "Supervised Classification", "LDA and Logistic Regression",
  7, "Supervised Classification", "Tree-Based Methods",
  8, "Supervised Classification", "Support Vector Machines",
  9, "Deep Learning", "Neural Networks",
  10, "Conclusion", "Written", 
)
contents
```
### 1: Introduction



### 2: Preprocessing

## 2.1 Extract Data
```{r, warning=FALSE}
location = "https://raw.githubusercontent.com/BrandonYChan/Statistical-Learning/main/Student_performance_10k.csv"

(df <- read_csv(location, show_col_types = FALSE, n_max=10000))
```

## 2.2 Clean Data
```{r}
df2 <- df |>
       rename(
         "has_subsidized_lunch" = "lunch",
         "has_prepared" = "test_preparation_course",
         "is_male" = "gender",
         "parent_education" = "parental_level_of_education",
       ) |> 
       mutate(
         is_male = as.integer(if_else(is_male =="male",1,0)),
         race_ethnicity = as.factor(str_sub(df$race_ethnicity, -1, -1)),
         parent_education = as.factor(parent_education),
         
         gpa_letter = as.factor(if_else(gpa_letter == "Fail", "F", gpa_letter)),    #CLS-FIX
         
         gpa_desirable = as.factor(if_else(gpa_letter %in% c("Fail","D","C"), "UnDesirable", "Desirable"))
         
         
       ) |>
       select(-c("roll_no"))   #CLS-FIX
```

## 2.3 Interpolate NA values
```{r}
#Remove Any Duplicates
df3 <- distinct(df2)


sum(is.na(df3))


#Sometimes GPA_Letter is missing --> Fill Using GPA
df3 <- df3 |>
        mutate(
          gpa_letter = case_when(
          !is.na(gpa_letter) ~ gpa_letter,
          gpa >= 80 ~ "A",
          gpa >= 62.5 ~ "B",
          gpa >= 50 ~ "C",
          gpa >= 37.5 ~ "D",
          gpa < 37.5 ~ "F",
          TRUE ~ sample(c("A","B","C","D","F"),1)
        ))


#Sometimes GPA is missing --> Estimate Using Letter
df3 <- df3 |>
        mutate(
          gpa = case_when(
          !is.na(gpa) ~ gpa,
          gpa_letter == "A" ~ sample(80:100,1),
          gpa_letter == "B" ~ sample(63:79, 1),
          gpa_letter == "C" ~ sample(50:62,1),
          gpa_letter == "D" ~ sample(38:49,1),
          gpa_letter == "F" ~ sample(0:37,1),
          TRUE ~ sample(0:100,1)
        ))

sum(is.na(df3))



#Replace Missing NUMERIC Values With Column Mean
if (any(is.na(df3))) {
  df3[] <- lapply(df3, function(x) {
    if (is.numeric(x)) {
      replace(x, is.na(x), mean(x, na.rm = TRUE))
    } else {
      x
    }
  })
}

#Dropping Missing Factor Values
df3 <- df3[complete.cases(df3[c("race_ethnicity", "parent_education", "gpa_desirable", "gpa_letter")]), ]   ###CLS-FIX
```

## 2.4 Display Variable Distributions
```{r}
#Display Numeric
df_numeric <- df3 |> select(where(is.numeric)) |> pivot_longer(cols = everything(), names_to = "variable", values_to = "value")
ggplot(df_numeric, aes(x = value)) + geom_histogram(bins = 30) +  facet_wrap(~ variable, scales = "free")  

#Display Factor
df_factor <- df3 |> select(where(is.factor)) |> pivot_longer(cols = everything(), names_to = "variable", values_to = "value") 
ggplot(df_factor, aes(x = value)) + geom_bar() + facet_wrap(~ variable, scales = "free") 
```

## 2.5 Format For Classification + Regression
```{r}

df4 <- df3 |> bind_cols(as.data.frame(model.matrix(~ `race_ethnicity` + `parent_education` - 1, data = df3))) |>
                 select(-c("race_ethnicity","parent_education"))

df4
```


## 2.6 Class Balancing
```{r}

table(df4$gpa_desirable)

min_class_size <- min(table(df4$gpa_desirable))


#df4 <- df4 |>
#  group_by(gpa_desirable) |>
#  sample_n(min_class_size) %>%
#  ungroup()

# Check the new class distribution

```



### 2.7 Split Into Train + Test Sets
```{r}
set.seed(10)
#Splits Rows
train_index <- sample(1:nrow(df4), nrow(df4)*0.8)

df_train <- df4[train_index,]
df_test <- df4[-train_index,]
```

# 3: Data Exploration with Unsupervised Methods (PCA and Clustering)

## 3.1 PCA: Generate Principle Components Train + Test
```{r}

# Find Principal Components 
pc <- prcomp(df_train |> select(-c("gpa_desirable","gpa_letter")), scale=TRUE)   #CLS-FIX

pc2 <- prcomp(df_test |> select(-c("gpa_desirable","gpa_letter")), scale=TRUE)  #CLS-FIX

#Highlight Distribution Of TRAIN GPA in PC
gpa_colors <- as.factor(df_train$gpa_letter)
plot(pc$x, col = gpa_colors, pch=20, cex=2)
legend("topright", legend = levels(gpa_colors), 
       fill = 1:length(levels(gpa_colors)), 
       pch = 20, title = "GPA Letter")
```

## 3.2 Impact of Variables on the First Principle Component
```{r}
pc1 <- pc$rotation[,1]
(pc1_loading_df <- cbind(names(pc1), as_data_frame(pc1)) |>
                  rename("names" = "names(pc1)", "pc1_loading" = "value") |> 
                  arrange(desc(abs(pc1_loading))))
```

## 3.3 Proportion of Variance Explained (PVE)
```{r}
pve <- pc$sdev^2 / sum(pc$sdev^2)   

(pve_df <- as_data_frame(cbind(1:length(pve), pve)) %>% # Show PVE for each number of principal components
  mutate(cumulative_pve = cumsum(pve)) %>% # Cumulative PVE for each additional PC 
  rename("num_principal_components" = "V1"))
```

## 3.4 Projecting Data Onto the First 2 Principal Components
```{r}
biplot(pc, scale = 0, cex=0.4)
```
## 3.6 Ideal Number of Clusters
```{r}
# within_cluster_sum_squares <- c()
# 
# # Calculate wcss for each number of clusters
# for(k in 1:10){
#   clusters <- kmeans(pc$x[,1:2], centers = k, nstart=100)
#   within_cluster_sum_squares[k] = clusters$tot_withinss
# }
# 
# plot(x=1:10, y=within_cluster_sum_squares)

# Elbow method did not work 

# Silhoutte Method

pc_1_2 <- pc$x[,1:2] # First 2 principal components
avg_scores = c()

# Find silhouette scores for 2:10 clusters 
for(k in 2:10){
  clusters = kmeans(pc_1_2, centers=k, nstart=25) 
  scores <- silhouette(clusters$cluster, dist(pc_1_2))
  avg_scores[k-1] = mean(scores[,3])
}

# Choose where there is a peak on the graph 
plot(x=2:10, y=avg_scores, type="b", xlab="k", ylab="Silhouette Score", main="Suggests 3 Clusters")
points(x=3, y=avg_scores[2], col="red", cex=2, pch=20) # Peak at k=3

```
## 3.7 K-Means Clustering on PC1 and PC2
```{r} 
km <- kmeans(pc$x[,1:2], 3, nstart=90) 
plot(pc$x, col = (km$cluster +1), pch=20, cex=2)
```





# 4: Variable Selection

## 4.1 Comparing Metrics
```{r}
par(mfrow=c(1,3))

# Note: djusted plots to start from 3 predictors; looks cleaner 

# Best subset selection
best_subset <- regsubsets(gpa~., df_train |> select(-c("gpa_letter", "gpa_desirable")), nvmax = ncol(df_train))
bs_summary <- summary(best_subset)

# Number of predictors in dataframe used 
n_preds_uncorr <- ncol(df_train)-3

# BIC plot
plot(3:n_preds_uncorr, bs_summary$bic[3:n_preds_uncorr], type="b", xlab="Number of predictors", ylab="BIC")
points(which.min(bs_summary$bic), min(bs_summary$bic), col="red", cex=2, pch=20) 

# AIC plot
plot(3:n_preds_uncorr, bs_summary$cp[3:n_preds_uncorr], type="b", xlab="Number of predictors", ylab="AIC")
points(which.min(bs_summary$cp), min(bs_summary$cp), col="red", cex=2, pch=20)
# lines(seq_along(bs_summary$cp), seq_along(bs_summary$cp)+1, col="gray", lty=2)

# Adjusted R^2 plot
plot(3:n_preds_uncorr, bs_summary$adjr2[3:n_preds_uncorr], type="b", xlab="Number of predictors", ylab="R^2")
points(which.max(bs_summary$adjr2), max(bs_summary$adjr2), col="red", cex=2, pch=20)
```

## 4.2 Extracting Key Variables
```{r}
tibble(Best_BIC = which.min(bs_summary$bic), Best_Cp = which.min(bs_summary$cp), Best_Adjusted_Rsq = which.max(bs_summary$adjr2))


#BIC HEAVILY PENALIZED COMPLEXITY, BUT ADRSQ, AND CP BOTH SEEM TO BE OKAY WITH HIGHER, AND THE FACT LOTS OF THESE ARE ONE HOT VARIABLES MAKES ME THINK SLIGHTLY MORE IS OKAY, MAYBE WE JUST CHOOISE SOMETHING RANDOM FOR NOW, USE IT TO MOVE ON, THEN COME BACK

p = 6

chosen_model <- bs_summary$which[p, ]
chosen_predictors = gsub("`", "", names(chosen_model[chosen_model == TRUE]))
chosen_predictors

```

## 4.3 Filtering Data Columns
```{r}

#UPDATING TRAIN + TEST TO ONLY INCLUDE IMPORTANT VARIABLES
selected_train <- df_train[, c("gpa","gpa_letter", "gpa_desirable", chosen_predictors[2:length(chosen_predictors)])]
selected_test <- df_test[, c("gpa","gpa_letter", "gpa_desirable",chosen_predictors[2:length(chosen_predictors)])]

colnames(selected_train) <- gsub(" ", "_", colnames(selected_train)) 
colnames(selected_train) <- gsub("'", "", colnames(selected_train))

colnames(selected_test) <- gsub(" ", "_", colnames(selected_test)) 
colnames(selected_test) <- gsub("'", "", colnames(selected_test))



colnames(selected_train)
```


### 5: Supervised Regression - Linear + Non Linear




## 5.1 Choosing Most Effective Linear Method
```{r}


train_x <- as.matrix(selected_train[, !colnames(selected_train) %in% c("gpa", "gpa_letter", "gpa_desirable")])  #FIX-CLS
train_y <- as.matrix(selected_train$gpa)

test_x <- as.matrix(selected_test[, !colnames(selected_test) %in% c("gpa", "gpa_letter", "gpa_desirable")])  #FIX-CLS
test_y <- as.matrix(selected_test$gpa)

#Linear Model
linear_model <- lm(gpa ~ ., data=selected_train[,!colnames(selected_train) %in% c("gpa_letter", "gpa_desirable")])  #FIX-CLS
linear_preds <- predict(linear_model, selected_test[,!colnames(selected_train) %in% c("gpa_letter", "gpa_desirable")])  #FIX-CLS
RMSE_linear = RMSE(test_y, linear_preds)
MAE_linear = MAE(test_y, linear_preds)

#LASSO:
optimal_lambda <- cv.glmnet(train_x, train_y, alpha=1)$lambda.min
lasso_model <- glmnet(train_x, train_y, alpha=1, lambda=optimal_lambda)
lasso_predictions <- predict(lasso_model, newx=test_x, s=optimal_lambda)
RMSE_lasso =  RMSE(test_y, lasso_predictions)
MAE_lasso = MAE(test_y, lasso_predictions)

#Ridge
optimal_lambda <- cv.glmnet(train_x, train_y, alpha=0)$lambda.min
ridge_model <- glmnet(train_x, train_y, alpha=0, lambda=optimal_lambda)
ridge_predictions <- predict(ridge_model, newx=test_x, s=optimal_lambda)
RMSE_ridge = RMSE(test_y, ridge_predictions)
MAE_ridge = MAE(test_y, ridge_predictions)

tribble(~"Loss Type", ~"Unregularised Linear", ~"LASSO", ~"Ridge",
        "RMSE", RMSE_linear, RMSE_lasso, RMSE_ridge,
        "MAE", MAE_linear, MAE_lasso, MAE_ridge
        )

```


## 5.2 Extracting Insight And Evaluating Chosen Linear Method
```{r}
# LASSO regression performs the best of these????????????//

#THEREFORE OUR BEST MODEL (AND INSIGHT FROM COEFFICIENTS COMES FROM LASSO)
coef(lasso_model)

accuracy_within_10 <- mean(abs(selected_test$gpa -lasso_predictions) < 10)
accuracy_within_10


#LASSO MODEL EVALUATION:
error_colors <- as.factor(abs(selected_test$gpa - lasso_predictions) < 10)
plot(pc2$x, col = error_colors, pch=20, cex=2)
legend("topright", legend = levels(error_colors), 
       fill = 1:length(levels(error_colors)), 
       pch = 20, title = "Reasonable Prediction")
```


## 5.3 Non-linear Regression

```{r}
# Polynomial Regression

# Natural Splines

# Generalized Additive Model (GAM)

```







### 6: Supervised Classification - LDA + Logistic Regression

## 6.1 LDA Model Creation
```{r}
lda_model <- lda(gpa_desirable ~ ., data = selected_train |> select(-c("gpa", "gpa_letter")))

lda_preds <- predict(lda_model, newdata = selected_test |> select(-c("gpa", "gpa_letter")))

lda_class <- lda_preds$class
table(lda_class, selected_test$gpa_desirable)


accuracy <- mean(lda_class == selected_test$gpa_desirable)
print(accuracy)
summary(lda_model)

#FIND DISCRIMINANT SCORE???

#IMPROVE???
```

## 6.2 LDA Model Evaluation
```{r}
#LDA MODEL EVALUATION:
error_colors <- as.factor(selected_test$gpa_desirable == lda_class)

plot(pc2$x, col = error_colors, pch=20, cex=2)
legend("topright", legend = levels(error_colors),
       fill = 1:length(levels(error_colors)),
       pch = 20, title = "Reasonable Prediction")


##INTERESTING PLOT!!!!!!!!!!!!!!!!!!!!!
predicted_probs <- lda_preds$posterior
colnames(predicted_probs) <- levels(selected_test$gpa_desirable)
```


## 6.3 Logistic Model Creation

```{r, include=FALSE}


#LOGISTIC REGRESSION

library(nnet)
 
log_reg_model <- multinom(gpa_desirable ~ ., data = selected_train |> select(-c("gpa","gpa_letter")))
 
#log_reg_pred_probs <- predict(log_reg_model, newdata = test_cls, type = "probs")

log_reg_pred_class <- predict(log_reg_model, newdata = selected_test |> select(-c("gpa","gpa_letter")), type = "class")
 
 
log_reg_accuracy <- mean(log_reg_pred_class == selected_test$gpa_desirable)

```

## 6.4 Logistic Model Evaluation
```{r}
 #LOGISTIC MODEL EVALUATION:
error_colors <- as.factor(log_reg_pred_class == selected_test$gpa_desirable)
plot(pc2$x, col = error_colors, pch=20, cex=2)
legend("topright", legend = levels(error_colors),
       fill = 1:length(levels(error_colors)),
       pch = 20, title = "Reasonable Prediction")

table(log_reg_pred_class, selected_test$gpa_desirable)

print(log_reg_accuracy)
```




### 7: Supervised Classification - Tree-Based Methods


## 7.1 Decision Tree
```{r}
library(tree)



selected_train
train_input <- selected_train |> select(-gpa, -gpa_letter) 
decision_tree <- tree(gpa_desirable ~ ., train_input)


summary(decision_tree)
plot(decision_tree)
text(decision_tree)

```

## 7.2 Random Forest Creation

```{r}
random_forest_model <- randomForest(gpa_desirable~., train_input, ntree=500, importance=TRUE)
summary(random_forest_model)
```

## 7.3 Random Forest Evaluation
```{r}
random_forest_model$confusion
importance(random_forest_model)
varImpPlot(random_forest_model)
```



# 8: Supervised Classification - Support Vector Machines


## 8.1 Comparing SVM Models
```{r}
# # This won't fully run on my laptop (been going 10 mins) so it won't knit
# 

#IF YOU LAPTOP IS STRUGGLING WE DONT HAVE TO USE BEST WE CAN JUST USE SOME DEFAULT IE: 1
# (SEE BELOW)

# #SVM
 library(e1071)
# 
try = seq(1,1 #10
          ,by=1)

# 
# #ATTEMPTING TO DETERMINE THE RELATIONSHIP OF DATA (IE LINEAR,QUADRATIC,RADIAL)
# 
# #from PC, we expect Linear Model To Work Well
# 
# 
train <- selected_train |> select(-c("gpa", "gpa_letter"))
test <- selected_test |> select(-c("gpa", "gpa_letter"))


selected_train
best <- tune(svm, gpa_desirable ~ ., data = train, ranges = list(cost = try),kernel="linear")$best.parameters$cost
 linear_kernel_svm <- svm(gpa_desirable ~ .,data=train, cost=best, kernel="linear")
 lin_preds <- predict(linear_kernel_svm, test, type="class") != test$gpa_desirable
 lin_mis <- mean(lin_preds)

 best <- tune(svm, gpa_desirable ~ ., data = train, ranges = list(cost = try), kernel="radial")$best.parameters$cost

 radial_kernel_svm <- svm(gpa_desirable ~ .,data=train, cost=best, kernel="radial")
 rad_mis <- mean(predict(radial_kernel_svm, test, type="class") != test$gpa_desirable)
  
 
  best <- tune(svm, gpa_desirable ~ ., data = train, ranges = list(cost = try), kernel="polynomial",degree=2)$best.parameters$cost
 quadratic_kernel_svm <- svm(gpa_desirable ~ .,data=train, cost=best, kernel="polynomial",degree=2)
 quad_mis <- mean(predict(quadratic_kernel_svm, test, type="class") != test$gpa_desirable)
# 
 tribble(~"LINEAR MIS", ~"RADIAL MIS", ~"QUAD MIS",
         lin_mis, rad_mis, quad_mis)
```

## 8.2 Display
```{r}
 error_colors <- as.factor(!lin_preds)
 plot(pc2$x, col = error_colors, pch=20, cex=2)
 legend("topright", legend = levels(error_colors), 
        fill = 1:length(levels(error_colors)), 
        pch = 20, title = "Reasonable Prediction")
```


# 9 - Deep Learning
```{r}
# Moved installations + imports here because they take a while 

# if(!require("keras")) {install.packages("keras")}
# reticulate::install_python(version = '3.11.9')
# install_keras(method = "virtualenv")

library(keras)


dim(selected_train)

# nn <- keras_model_sequential() %>% 
#   layer_dense(units=50, activation="relu", input_shape=dim(selected_train)) %>% 
#   layer_dropout(rate=0.1) %>% 
#   layer_dense(units=25, activation="relu", input_shape=c(50)) %>% 
#   layer_dense(units=unique(selected_train$gpa), activation="softmax")


```





### 10 - Conclusions


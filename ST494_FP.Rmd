---
title: "Predicting and Understanding NBA Career Length"
author: "Brandon Chan + Lucas Duncan"
date: "2025-03-07"
output: html_document
---

```{r}
#NOTES TO SELF:::::::::
  
#-GPA is sum of 4 Classes, So any model cant use all 4
```

## 2.0: Imports
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

if(! require("caret")){install.packages("caret")}
if(! require("tidyverse")){install.packages("tidyverse")}
if(! require("ISLR2")){install.packages("ISLR2")}
if(! require("boot")){install.packages("boot")}
if(! require("MASS") ){ install.packages("MASS") }
if(! require("leaps") ){ install.packages("leaps") }
if(! require("glmnet") ){ install.packages("glmnet") }
if(! require("pls") ){ install.packages("pls") }
if(! require("splines")) {install.packages("splines")}
if(! require("e1071")){ install.packages("e1071")}
if(! require("pROC")){ install.packages("pROC")}
if(! require("class")){ install.packages("class")}
if(! require("reshape2")){ install.packages("reshape2")}
if(! require("DAAG")){ install.packages("DAAG")}
if(! require("ROCR")){ install.packages("ROCR")}
if(! require("tree")){ install.packages("tree")}
if(! require("randomForest")){ install.packages("randomForest")}


library(tidyverse)
library(e1071)
library(MASS)
library(tidyverse)
library(caret)
library(leaps)
library(tidyr)
library(ggplot2)
library(glmnet)
library(ROCR)
library(pROC)
library(randomForest)
library(tree)
select <- dplyr::select
```


## Executive Summary and Contents
```{r}
contents <- tribble(
  ~Index, ~Category, ~Method,
  1, "Introduction", "Written",
  2, "Preprocessing", "Data Description and Encoding",
  3, "Unsupervised Learning/Analysis", "PCA and Clustering",
  4, "Preprocessing", "Variable Selection",
  5, "Supervised Regression", "Linear and Non-Linear Regression",
  6, "Supervised Classification", "LDA/QDA and Logistic Regression",
  7, "Supervised Classification", "Tree-Based Methods",
  8, "Supervised Classification", "Support Vector Machines",
  9, "Deep Learning", "Neural Networks",
  10, "Conclusion", "Written", 
)
contents
```


### 2: Preprocessing

## 2.1 Extract Data
```{r, warning=FALSE}
location = "https://raw.githubusercontent.com/BrandonYChan/Statistical-Learning/main/Student_performance_10k.csv"

(df <- read_csv(location, show_col_types = FALSE, n_max=10000))
```

## 2.2 Clean Data
```{r}
df2 <- df |>
       rename(
         "has_subsidized_lunch" = "lunch",
         "has_prepared" = "test_preparation_course",
         "is_male" = "gender",
         "parent_education" = "parental_level_of_education",
         "gpa" = "total_score",
         "gpa_letter" = "grade"
       ) |> 
       mutate(
         is_male = as.integer(if_else(is_male =="male",1,0)),
         race_ethnicity = as.factor(str_sub(df$race_ethnicity, -1, -1)),
         parent_education = as.factor(parent_education),
         gpa = gpa/4,
         gpa_letter = as.factor(if_else(gpa_letter == "Fail", "F", gpa_letter))
       ) |>
       select(-c("roll_no","science_score","writing_score"))
```

## 2.3 Interpolate NA values
```{r}
#Remove Any Duplicates
df3 <- distinct(df2)

#Replace Missing NUMERIC Values With Column Mean
if (any(is.na(df3))) {
  df3[] <- lapply(df3, function(x) {
    if (is.numeric(x)) {
      replace(x, is.na(x), mean(x, na.rm = TRUE))
    } else {
      x
    }
  })
}

#Dropping Missing Factor Columns
df3 <- df3[complete.cases(df3[c("race_ethnicity", "parent_education", "gpa_letter")]), ]
```

## 2.4 Display Variable Distributions
```{r}
#Display Numeric
df_numeric <- df3 |> select(where(is.numeric)) |> pivot_longer(cols = everything(), names_to = "variable", values_to = "value")
ggplot(df_numeric, aes(x = value)) + geom_histogram(bins = 30) +  facet_wrap(~ variable, scales = "free")  

#Display Factor
df_factor <- df3 |> select(where(is.factor)) |> pivot_longer(cols = everything(), names_to = "variable", values_to = "value") 
ggplot(df_factor, aes(x = value)) + geom_bar() + facet_wrap(~ variable, scales = "free") 
```

## 2.5 Format For Classification + Regression
```{r}

df4 <- df3 |> bind_cols(as.data.frame(model.matrix(~ `race_ethnicity` + `parent_education` - 1, data = df3))) |>
                 select(-c("race_ethnicity","parent_education"))
```


## 2.6 Split Into Train + Test Sets
```{r}
set.seed(10)
#Splits Rows
train_index <- sample(1:nrow(df4), nrow(df4)*0.8)

df_train <- df4[train_index,]
df_test <- df4[-train_index,]
```



# 3 Data Exploration with Unsupervised Methods 


## 3.1 Principal Component Analysis


## 3.1.1 Generate Principle Components Train + Test
```{r}

# Find Principal Components 
pc <- prcomp(df_train |> select(-c("gpa_letter")), scale=TRUE)

pc2 <- prcomp(df_test |> select(-c("gpa_letter")), scale=TRUE)

#Highlight Distribution Of TRAIN GPA in PC
gpa_colors <- as.factor(df_train$gpa_letter)
plot(pc$x, col = gpa_colors, pch=20, cex=2)
legend("topright", legend = levels(gpa_colors), 
       fill = 1:length(levels(gpa_colors)), 
       pch = 20, title = "GPA Letter")

# Not supposed to look at the test set

#Highlight Distribution of TEST GPA in PC
# gpa_colors <- as.factor(df_test$gpa_letter)
# plot(pc2$x, col = gpa_colors, pch=20, cex=2)
# legend("topright", legend = levels(gpa_colors), 
#        fill = 1:length(levels(gpa_colors)), 
#        pch = 20, title = "GPA Letter")
```



#3.1.2 Show Impact of Variables on the First Principle Component
```{r}
pc1 <- pc$rotation[,1]
(pc1_loading_df <- cbind(names(pc1), as_data_frame(pc1)) |>
                  rename("names" = "names(pc1)", "pc1_loading" = "value") |> 
                  arrange(desc(abs(pc1_loading))))
```

#3.1.3 Show Proportion of Variance Explained (PVE)
```{r}
pve <- pc$sdev^2 / sum(pc$sdev^2)   

(pve_df <- as_data_frame(cbind(1:length(pve), pve)) %>% # Show PVE for each number of principal components
  mutate(cumulative_pve = cumsum(pve)) %>% # Cumulative PVE for each additional PC 
  rename("num_principal_components" = "V1"))
```

#3.1.4 Show projections of data onto first 2 PCs
```{r}
biplot(pc, scale = 0, cex=0.4)
```


#3.2 K-Means Clustering on PC1 and PC2
```{r}
km <- kmeans(pc$x, 6, nstart=50)
plot(pc$x, col = (km$cluster +1), pch=20, cex=2)
```





# Variable Selection

```{r}
par(mfrow=c(1,3))

# Best subset selection
best_subset <- regsubsets(gpa~., df_train |> select(-c("gpa_letter")), nvmax = ncol(df_train))
(bs_summary <- summary(best_subset))

# BIC plot
plot(seq_len(length(bs_summary$bic)), bs_summary$bic, type="b", xlab="Number of predictors", ylab="BIC")
points(which.min(bs_summary$bic), min(bs_summary$bic), col="red", cex=2, pch=20) 

# cp plot
plot(seq_len(length(bs_summary$cp)), bs_summary$cp, type="b", xlab="Number of predictors", ylab="CP",ylim=c(0,20))
points(which.min(bs_summary$cp), min(bs_summary$cp), col="red", cex=2, pch=20)
# lines(seq_along(bs_summary$cp), seq_along(bs_summary$cp)+1, col="gray", lty=2)

# Adjusted R^2 plot
plot(seq_len(length(bs_summary$adjr2)), bs_summary$adjr2, type="b", xlab="Number of predictors", ylab="R^2",ylim=c(0.45,1))
points(which.max(bs_summary$adjr2), max(bs_summary$adjr2), col="red", cex=2, pch=20)
```

```{r}
tibble(Best_BIC = which.min(bs_summary$bic), Best_Cp = which.min(bs_summary$cp), Best_Adjusted_Rsq = which.max(bs_summary$adjr2))


#BIC HEAVILY PENALIZED COMPLEXITY, BUT ADRSQ, AND CP BOTH SEEM TO BE OKAY WITH HIGHER, AND THE FACT LOTS OF THESE ARE ONE HOT VARIABLES MAKES ME THINK SLIGHTLY MORE IS OKAY, MAYBE WE JUST CHOOISE SOMETHING RANDOM FOR NOW, USE IT TO MOVE ON, THEN COME BACK

p = 6

chosen_model <- bs_summary$which[p, ]
chosen_predictors = gsub("`", "", names(chosen_model[chosen_model == TRUE]))
chosen_predictors

```


```{r}

#UPDATING TRAIN + TEST TO ONLY INCLUDE IMPORTANT VARIABLES
df_train <- df_train[, c("gpa","gpa_letter", chosen_predictors[2:length(chosen_predictors)])]
df_test <- df_test[, c("gpa","gpa_letter", chosen_predictors[2:length(chosen_predictors)])]


df_test
```


#LINEAR REGRESSION




#   Choosing Most Effective Method
```{r}

#Generating Matrix Form
train_x <- as.matrix(df_train[, -c(1,2)])
train_y <- as.matrix(df_train$gpa)

test_x <- as.matrix(df_test[, -c(1,2)])
test_y <- as.matrix(df_test$gpa)

#Linear Model
linear_model <- lm(gpa ~ ., data=df_train[,-c(2)])
linear_preds <- predict(linear_model, df_test[,-c(2)])
RMSE_linear = RMSE(test_y, linear_preds)

#LASSO:
optimal_lambda <- cv.glmnet(train_x, train_y, alpha=1)$lambda.min
lasso_model <- glmnet(train_x, train_y, alpha=1, lambda=optimal_lambda)
lasso_predictions <- predict(lasso_model, newx=test_x, s=optimal_lambda)
RMSE_lasso =  RMSE(test_y, lasso_predictions)

#Ridge
optimal_lambda <- cv.glmnet(train_x, train_y, alpha=0)$lambda.min
ridge_model <- glmnet(train_x, train_y, alpha=0, lambda=optimal_lambda)
ridge_predictions <- predict(ridge_model, newx=test_x, s=optimal_lambda)
RMSE_ridge = RMSE(test_y, ridge_predictions)

tribble(~"Unregularised Linear RMSE", ~"LASSO RMSE", ~"Ridge RMSE",
        RMSE_linear, RMSE_lasso, RMSE_ridge)

```


# Extracting Insight And Evaluating Chosen Method
```{r}
# LASSO regression performs the best of these

#THEREFORE OUR BEST MODEL (AND INSIGHT FROM COEFFICIENTS COMES FROM LASSO)
coef(lasso_model)


#LASSO MODEL EVALUATION:
error_colors <- as.factor(abs(df_test$gpa - lasso_predictions) < 10)
plot(pc2$x, col = error_colors, pch=20, cex=2)
legend("topright", legend = levels(error_colors), 
       fill = 1:length(levels(error_colors)), 
       pch = 20, title = "Reasonable Prediction")



```

#NON-LINEAR REGRESSION

```{r}
# Polynomial Regression

# Natural Splines

# Generalized Additive Model (GAM)

```


#LDA + QDA
```{r}
lda_model <- lda(gpa_letter ~ ., data = df_train |> select(-c("gpa")))

lda_preds <- predict(lda_model, newdata = df_test |> select(-c("gpa")))

lda_class <- lda_preds$class
table(lda_class, df_test$gpa_letter)


accuracy <- mean(lda_class == df_test$gpa_letter)
print(accuracy)
summary(lda_model)
```


```{r}
#LDA MODEL EVALUATION:
error_colors <- as.factor(df_test$gpa_letter == lda_class)


plot(pc2$x, col = error_colors, pch=20, cex=2)
legend("topright", legend = levels(error_colors),
       fill = 1:length(levels(error_colors)),
       pch = 20, title = "Reasonable Prediction")

predicted_probs <- lda_preds$posterior
colnames(predicted_probs) <- levels(df_test$gpa_letter)
```


```{r}
#QDA:

#READ:
#some groups are too small size for QDA to work (its an actual error), so as a group we gotta decide how we want to handle that

# I think we just get rid of it and group LDA with logistic regression under the same header 
```


```{r, include=FALSE}


#LOGISTIC REGRESSION

library(nnet)
 
log_reg_model <- multinom(gpa_letter ~ ., data = df_train |> select(-c("gpa")))
 
#log_reg_pred_probs <- predict(log_reg_model, newdata = test_cls, type = "probs")

log_reg_pred_class <- predict(log_reg_model, newdata = df_test |> select(-c("gpa")), type = "class")
 
 
log_reg_accuracy <- mean(log_reg_pred_class == df_test$gpa_letter)
 
```

```{r}
 #LOGISTIC MODEL EVALUATION:
error_colors <- as.factor(log_reg_pred_class == df_test$gpa_letter)
plot(pc2$x, col = error_colors, pch=20, cex=2)
legend("topright", legend = levels(error_colors),
       fill = 1:length(levels(error_colors)),
       pch = 20, title = "Reasonable Prediction")


table(log_reg_pred_class, df_test$gpa_letter)


```

```{r}
print(log_reg_accuracy)
# summary(log_reg_model)
```

## Tree Based Methods

### Decision Tree

```{r}
library(tree)
train_input <- df_train %>% select(-c(gpa)) %>% rename(masters_degree="parent_educationmaster's degree") # This variable is messed up
decision_tree <- tree(gpa_letter ~ ., train_input)

summary(decision_tree)
plot(decision_tree)
text(decision_tree)

```

### Random Forest

```{r}
random_forest_model <- randomForest(gpa_letter~., train_input, ntree=500, importance=TRUE)
summary(random_forest_model)
```


```{r}
random_forest_model$confusion
importance(random_forest_model)
varImpPlot(random_forest_model)
```



## Support Vector Machines
```{r}
# # This won't fully run on my laptop (been going 10 mins) so it won't knit
# 

#IF YOU LAPTOP IS STRUGGLING WE DONT HAVE TO USE BEST WE CAN JUST USE SOME DEFAULT IE: 1
# (SEE BELOW)

# #SVM
 library(e1071)
# 
try = seq(1,1 #10
          ,by=1)

# 
# #ATTEMPTING TO DETERMINE THE RELATIONSHIP OF DATA (IE LINEAR,QUADRATIC,RADIAL)
# 
# #from PC, we expect Linear Model To Work Well
# 
# 
train <- df_train |> select(-c("gpa"))
test <- df_test |> select(-c("gpa"))
# 
best <- tune(svm, gpa_letter ~ ., data = train, ranges = list(cost = try),kernel="linear")$best.parameters$cost
print(best)
 linear_kernel_svm <- svm(gpa_letter ~ .,data=train, cost=best, kernel="linear")
 lin_preds <- predict(linear_kernel_svm, test, type="class") != test$gpa_letter
 lin_mis <- mean(lin_preds)

 best <- tune(svm, gpa_letter ~ ., data = train, ranges = list(cost = try), kernel="radial")$best.parameters$cost
 print(best)
 radial_kernel_svm <- svm(gpa_letter ~ .,data=train, cost=best, kernel="radial")
 rad_mis <- mean(predict(radial_kernel_svm, test, type="class") != test$gpa_letter)
  
 
  best <- tune(svm, gpa_letter ~ ., data = train, ranges = list(cost = try), kernel="polynomial",degree=2)$best.parameters$cost
  print(best)
 quadratic_kernel_svm <- svm(gpa_letter ~ .,data=train, cost=best, kernel="polynomial",degree=2)
 quad_mis <- mean(predict(quadratic_kernel_svm, test, type="class") != test$gpa_letter)
# 
 tribble(~"LINEAR MIS", ~"RADIAL MIS", ~"QUAD MIS",
         lin_mis, rad_mis, quad_mis)
```

```{r}
 error_colors <- as.factor(!lin_preds)
 plot(pc2$x, col = error_colors, pch=20, cex=2)
 legend("topright", legend = levels(error_colors), 
        fill = 1:length(levels(error_colors)), 
        pch = 20, title = "Reasonable Prediction")
```












---
title: "Predicting GPA and Interpreting Results"
author: "Brandon Chan + Lucas Duncan"
date: "2025-03-07"
output: 
  html_document: 
    df_print: kable 
  pdf_document: default
---

```{r}
#NOTES TO SELF:::::::::
  
#-GPA is sum of 4 Classes, So any model cant use all 4

# How we can predict GPA, factors that impact it, what we recommend for students to raise their GPA, institutions to evaluate GPA, and systematic issues. 

```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(tibble.print_min = 4, tibble.print_max = 10) 

if(! require("caret")){install.packages("caret")}
if(! require("tidyverse")){install.packages("tidyverse")}
if(! require("ISLR2")){install.packages("ISLR2")}
if(! require("boot")){install.packages("boot")}
if(! require("MASS") ){ install.packages("MASS") }
if(! require("leaps") ){ install.packages("leaps") }
if(! require("glmnet") ){ install.packages("glmnet") }
if(! require("pls") ){ install.packages("pls") }
if(! require("splines")) {install.packages("splines")}
if(! require("e1071")){ install.packages("e1071")}
if(! require("pROC")){ install.packages("pROC")}
if(! require("class")){ install.packages("class")}
if(! require("reshape2")){ install.packages("reshape2")}
if(! require("DAAG")){ install.packages("DAAG")}
if(! require("ROCR")){ install.packages("ROCR")}
if(! require("tree")){ install.packages("tree")}
if(! require("rpart")){ install.packages("rpart")}
if(! require("rpart.plot")){ install.packages("rpart.plot")}
if(! require("randomForest")){ install.packages("randomForest")}
if(! require("gam")){ install.packages("gam")}

# MOVING IMPORTS WITH THE REST

# if(!require("keras")) {install.packages("keras")}
# reticulate::install_python(version = '3.11.9')
# install_keras(method = "virtualenv")


library(tidyverse)
library(e1071)
library(MASS)
library(tidyverse)
library(caret)
library(leaps)
library(tidyr)
library(ggplot2)
library(cluster)
library(glmnet)
library(ROCR)
library(pROC)
library(randomForest)
library(tree)
library(gam)
library(pander)
library(knitr)
library(nnet)
library(rpart)
library(e1071)
library(keras)

select <- dplyr::select


set.seed(10)
```

# Executive Summary and Contents 

```{r}
contents <- tribble(
  ~Index, ~Category, ~Method,
  1, "Introduction", "Written",
  2, "Preprocessing", "Data Description and Encoding",
  3, "Unsupervised Learning/Analysis", "PCA and Clustering",
  4, "Preprocessing", "Variable Selection",
  5, "Supervised Regression", "Linear and Non-Linear Regression",
  6, "Supervised Linear Classification", "LDA and Logistic Regression",
  7, "Supervised Non-Linear Classification", "Tree-Based Methods, Support Vector Machines",
  8, "Deep Learning", "Neural Networks",
  9, "Conclusion", "Written", 
) 
contents
```

# Introduction

By analyzing different factors that contribute to student performance, we aim to provide insight into how institutions can improve educational and environmental factors that foster student success. This is an issue that we can relate to directly. As students ourselves, we understand that success in academics is influenced by several factors that include preparation and ability, but go beyond to include factors outside of school.  

```{r, warning=FALSE, include=FALSE}
# 2.1 Extract Data
location = "https://raw.githubusercontent.com/BrandonYChan/Statistical-Learning/main/Student_performance_10k.csv"

df <- read_csv(location, show_col_types = FALSE)
```

#We start by cleaning our data; renaming columns for clarity, converting column types, generating attributes, and filtering rows and columns. We filter duplicate rows, and interpolate using column means for missing numerical variables, we fill variables where possible to fix inconsistencies between gpa and gpa_letter, then drop the remaining rows missing key factor variables.

```{r, include=FALSE}
# 2.2 Clean Data

df2 <- df |>
       rename(
         "has_subsidized_lunch" = "lunch",
         "has_prepared" = "test_preparation_course",
         "is_male" = "gender",
         "parent_education" = "parental_level_of_education",
       ) |> 
       mutate(
         is_male = as.integer(if_else(is_male =="male",1,0)),
         race_ethnicity = as.factor(str_sub(df$race_ethnicity, -1, -1)),
         parent_education = as.factor(parent_education),
         has_prepared = as.integer(has_prepared),
         gpa_letter = as.factor(if_else(gpa_letter == "Fail", "F", gpa_letter)),    
         
         gpa_desirable = as.factor(if_else(gpa_letter %in% c("Fail","D","C"), "UnDesirable", "Desirable"))
       ) |>
       select(-c("roll_no"))   
```

## The Student Performance Dataset

```{r}
# 2.3 Interpolate NA values

#Remove Any Duplicates
df3 <- distinct(df2)

#Sometimes GPA_Letter is missing --> Fill Using GPA
df3 <- df3 |>
        mutate(
          gpa_letter = case_when(
          !is.na(gpa_letter) ~ gpa_letter,
          gpa >= 80 ~ "A",
          gpa >= 62.5 ~ "B",
          gpa >= 50 ~ "C",
          gpa >= 37.5 ~ "D",
          gpa < 37.5 ~ "F",
          TRUE ~ sample(c("A","B","C","D","F"),1)
        ))

#Sometimes GPA is missing --> Estimate Using Letter
df3 <- df3 |>
        mutate(
          gpa = case_when(
          !is.na(gpa) ~ gpa,
          gpa_letter == "A" ~ sample(80:100,1),
          gpa_letter == "B" ~ sample(63:79, 1),
          gpa_letter == "C" ~ sample(50:62,1),
          gpa_letter == "D" ~ sample(38:49,1),
          gpa_letter == "F" ~ sample(0:37,1),
          TRUE ~ sample(0:100,1)
        ))

#Replace Missing NUMERIC Values With Column Mean
if (any(is.na(df3))) {
  df3[] <- lapply(df3, function(x) {
    if (is.numeric(x)) {
      replace(x, is.na(x), round(mean(x, na.rm = TRUE)))
    } else {
      x
    }
  })
}

#Dropping Missing Factor Values
df3 <- df3[complete.cases(df3[c("race_ethnicity", "parent_education", "gpa_desirable", "gpa_letter")]), ]   

head(df3, 2)
```

## Variable Distributions

#We now want to get more familier with our data, looking first at the numeric columns, we observe all seem approximately normal, which is to be expected given their nature. We note they all seem to be within acceptable range (0,100). We next analyze the boolean columns, none of which have one boolean option which dominated the data. Notably there seems to be an approximately equal number of males and females, which is a good sign that our sample is proportional to an entire group. Finally we analyze the true factors, eduction and race, and once again no one factor dominates the data, while factors are not exactly normally distributed, nor should they be, there is healthy variation between rows with no one factor representing too few rows.

```{r}
## 2.4 Display Variable Distributions

#Display Numeric
df_numeric <- df3 |> select(any_of(c("gpa", "recent_math_score", "recent_science_score", "recent_writing_score"))) |> pivot_longer(cols = everything(), names_to = "variable", values_to = "value")
ggplot(df_numeric, aes(x = value)) + geom_histogram(bins = 30) +  facet_wrap(~ variable, scales = "free")  

#Display Factor
df_factor <- df3 |> mutate(across(c("has_prepared", "has_subsidized_lunch", "is_male"), as.factor )) |> select(where(is.factor)) |>
                 pivot_longer(cols = everything(), names_to = "variable", values_to = "value") 
ggplot(df_factor, aes(x = value)) + geom_bar() + facet_wrap(~ variable, scales = "free") 
```

## Encoded Data Frame

From here, our next step in processing is to convert our factor variables into one hot notation to be used in predictive models. We modify both parent_eduction and race, as well as converting boolean factor variables into integers for use in models. 

```{r}
# 2.5 Format For Classification + Regression

df4 <- df3 |> 
  bind_cols(as.data.frame(model.matrix(~ `race_ethnicity` + `parent_education` - 1, data = df3))) |>
  select(-c("race_ethnicity","parent_education")) |>
  transform(has_subsidized_lunch = as.integer(has_subsidized_lunch), has_prepared=as.integer(has_subsidized_lunch), is_male=as.integer(is_male))

head(df4, 2)
```

```{r, include=FALSE}
# 2.6 Split Into Train + Test Sets

train_index <- sample(1:nrow(df4), nrow(df4)*0.8)

df_train <- df4[train_index,]
df_test <- df4[-train_index,]
```

# 3: Data Exploration with Unsupervised Methods (PCA and Clustering)

## 3.1 PCA: Generate Principle Components Train + Test
```{r}

# 3.1 PCA: Generate Principle Components Train + Test

# Find Principal Components 
pc <- prcomp(df_train |> select(-c("gpa_desirable","gpa_letter")), scale=TRUE)   

pc2 <- prcomp(df_test |> select(-c("gpa_desirable","gpa_letter")), scale=TRUE)  

#Highlight Distribution Of TRAIN GPA in PC
gpa_colors <- as.factor(df_train$gpa_letter)
plot(pc$x, col = gpa_colors, pch=20, cex=2)
legend("topright", legend = levels(gpa_colors), 
       fill = 1:length(levels(gpa_colors)), 
       pch = 20, title = "GPA Letter")
```

## 3.2 Impact of Variables on the First Principle Component
```{r}

# 3.2 Impact of Variables on the First Principle Component

pc1 <- pc$rotation[,1]
(pc1_loading_df <- cbind(names(pc1), as_data_frame(pc1)) |>
                  rename("names" = "names(pc1)", "pc1_loading" = "value") |> 
                  arrange(desc(abs(pc1_loading))))
```

## 3.3 Proportion of Variance Explained (PVE)

```{r}
# 3.3 Proportion of Variance Explained (PVE)

pve <- pc$sdev^2 / sum(pc$sdev^2)   

(pve_df <- as_data_frame(cbind(1:length(pve), pve)) |> # Show PVE for each number of principal components
  mutate(cumulative_pve = cumsum(pve)) |> # Cumulative PVE for each additional PC 
  rename("num_principal_components" = "V1"))
```

## 3.4 Projecting Data Onto the First 2 Principal Components

```{r}
# 3.4 Projecting Data Onto the First 2 Principal Components

biplot(pc, scale = 0, cex=0.4)
```

## 3.6 Ideal Number of Clusters

```{r, warning=FALSE}
# 3.6 Ideal Number of Clusters

# within_cluster_sum_squares <- c()
# 
# # Calculate wcss for each number of clusters
# for(k in 1:10){
#   clusters <- kmeans(pc$x[,1:2], centers = k, nstart=100)
#   within_cluster_sum_squares[k] = clusters$tot_withinss
# }
# 
# plot(x=1:10, y=within_cluster_sum_squares)

# Elbow method did not work 

# Silhoutte Method

pc_1_2 <- pc$x[,1:2] # First 2 principal components
avg_scores = c()

# Find silhouette scores for 2:10 clusters 
for(k in 2:10){
  clusters = kmeans(pc_1_2, centers=k, nstart=25) 
  scores <- silhouette(clusters$cluster, dist(pc_1_2))
  avg_scores[k-1] = mean(scores[,3])
}

# Choose where there is a peak on the graph 
plot(x=2:10, y=avg_scores, type="b", xlab="k", ylab="Silhouette Score", main="Suggests 4 Clusters")
points(x=4, y=avg_scores[3], col="red", cex=2, pch=20) # Peak at k=3

ideal_clusters <- 4
```
## 3.7 K-Means Clustering on PC1 and PC2

```{r} 
# 3.7 K-Means Clustering on PC1 and PC2

km <- kmeans(pc$x[,1:2], centers=ideal_clusters, nstart=90) 
plot(pc$x, col = (km$cluster +1), pch=20, cex=2)
```

# 4: Variable Selection

## Evaluating Criterion for Best Subset Selection

```{r, warning=FALSE, message=FALSE}

# 4.1 Comparing Metrics

par(mfrow=c(1,3))

# Note: djusted plots to start from 3 predictors; looks cleaner 

# Best subset selection
best_subset <- regsubsets(gpa~., df_train |> select(-c("gpa_letter", "gpa_desirable")), nvmax = ncol(df_train))
bs_summary <- summary(best_subset)

# Number of predictors in dataframe used 
n_preds_uncorr <- ncol(df_train)-3

# BIC plot
plot(3:n_preds_uncorr, bs_summary$bic[3:n_preds_uncorr], type="b", xlab="Number of predictors", ylab="BIC")
points(which.min(bs_summary$bic), min(bs_summary$bic), col="red", cex=2, pch=20) 

# AIC plot
plot(3:n_preds_uncorr, bs_summary$cp[3:n_preds_uncorr], type="b", xlab="Number of predictors", ylab="AIC")
points(which.min(bs_summary$cp), min(bs_summary$cp), col="red", cex=2, pch=20)

#SHOULD WE KEEP THIS?????? 
# lines(seq_along(bs_summary$cp), seq_along(bs_summary$cp)+1, col="gray", lty=2)

# Adjusted R^2 plot
plot(3:n_preds_uncorr, bs_summary$adjr2[3:n_preds_uncorr], type="b", xlab="Number of predictors", ylab="R^2")
points(which.max(bs_summary$adjr2), max(bs_summary$adjr2), col="red", cex=2, pch=20)
```

## Ideal Number of Variables for Each Criterion

```{r}
# 4.2 Extracting Key Variables

tibble(Best_BIC = which.min(bs_summary$bic), Best_Cp = which.min(bs_summary$cp), Best_Adjusted_Rsq = which.max(bs_summary$adjr2))
```

## Names of Ideal Predictors 

```{r}
p = 6  #currently arbitrarily chosen, better method???

chosen_model <- bs_summary$which[p, ]
chosen_predictors = gsub("`", "", names(chosen_model[chosen_model == TRUE]))
chosen_predictors
# We should make this output cleaner in the final edit 
```

```{r, include=FALSE}
# 4.3 Filtering Data Columns

#UPDATING TRAIN + TEST TO ONLY INCLUDE IMPORTANT VARIABLES
selected_train <- df_train[, c("gpa","gpa_letter", "gpa_desirable", chosen_predictors[2:length(chosen_predictors)])]
selected_test <- df_test[, c("gpa","gpa_letter", "gpa_desirable",chosen_predictors[2:length(chosen_predictors)])]

colnames(selected_train) <- gsub(" ", "_", colnames(selected_train)) 
colnames(selected_train) <- gsub("'", "", colnames(selected_train))

colnames(selected_test) <- gsub(" ", "_", colnames(selected_test)) 
colnames(selected_test) <- gsub("'", "", colnames(selected_test))

colnames(selected_train)
```


# 5: Supervised Regression - Linear + Non Linear

## 5.1 Choosing Most Effective Linear Method

```{r}
train_x <- as.matrix(selected_train[, !colnames(selected_train) %in% c("gpa", "gpa_letter", "gpa_desirable")])  
train_y <- as.matrix(selected_train$gpa)

test_x <- as.matrix(selected_test[, !colnames(selected_test) %in% c("gpa", "gpa_letter", "gpa_desirable")])  
test_y <- as.matrix(selected_test$gpa)

#Linear Model
linear_model <- lm(gpa ~ ., data=selected_train[,!colnames(selected_train) %in% c("gpa_letter", "gpa_desirable")])  
linear_preds <- predict(linear_model, selected_test[,!colnames(selected_train) %in% c("gpa_letter", "gpa_desirable")])  
RMSE_linear = RMSE(test_y, linear_preds)
MAE_linear = MAE(test_y, linear_preds)

#LASSO:
optimal_lambda <- cv.glmnet(train_x, train_y, alpha=1)$lambda.min
lasso_model <- glmnet(train_x, train_y, alpha=1, lambda=optimal_lambda)
lasso_predictions <- predict(lasso_model, newx=test_x, s=optimal_lambda)
RMSE_lasso =  RMSE(test_y, lasso_predictions)
MAE_lasso = MAE(test_y, lasso_predictions)

#Ridge
optimal_lambda <- cv.glmnet(train_x, train_y, alpha=0)$lambda.min
ridge_model <- glmnet(train_x, train_y, alpha=0, lambda=optimal_lambda)
ridge_predictions <- predict(ridge_model, newx=test_x, s=optimal_lambda)
RMSE_ridge = RMSE(test_y, ridge_predictions)
MAE_ridge = MAE(test_y, ridge_predictions)

tribble(~"Loss Type", ~"Unregularised Linear", ~"LASSO", ~"Ridge",
        "RMSE", RMSE_linear, RMSE_lasso, RMSE_ridge,
        "MAE", MAE_linear, MAE_lasso, MAE_ridge
        )

```


## 5.2 Extracting Insight And Evaluating Chosen Linear Method

### Predictor Coefficients

```{r}
# LASSO regression performs the best of these????????????//

#THEREFORE OUR BEST MODEL (AND INSIGHT FROM COEFFICIENTS COMES FROM LASSO)
coef(lasso_model)
```

### Accuracy Within 10% 

```{r}
accuracy_within_10 <- mean(abs(selected_test$gpa -lasso_predictions) < 10)
accuracy_within_10
```


```{r}
#LASSO MODEL EVALUATION:
error_colors <- as.factor(abs(selected_test$gpa - lasso_predictions) < 10)
plot(pc2$x, col = error_colors, pch=20, cex=2)
legend("topright", legend = levels(error_colors), 
       fill = 1:length(levels(error_colors)), 
       pch = 20, title = "Reasonable Prediction")
```


## 5.3 Non-linear Regression: GAM

```{r, include=FALSE}
# Cross Validation to Choose the Best One

chosen_preds_tidy <- preds_formula <- gsub(" ", "_", chosen_predictors[-c(1)])
preds_formula <- paste(chosen_preds_tidy, collapse=" + ")

gam_obj <- gam(as.formula(paste("gpa~", preds_formula)), data=selected_train)


scope_list <- list()

for(i in seq_along(chosen_preds_tidy)){
  predictor_name <- chosen_preds_tidy[i]
  
  length_unique <- nrow(distinct(selected_train[chosen_preds_tidy[i]]))
  
  if(length_unique > 2){
    formula_curr <- as.formula(paste("~1 +", predictor_name, "+", "s(",predictor_name, ", 2)", "+ s(", predictor_name, ", 3)"))
  }
  else{
    formula_curr <- as.formula(paste("~1 +", predictor_name))
  }
  scope_list[[predictor_name]] <- formula_curr
}

scope_list

step.Gam(gam_obj, scope=scope_list)

# summary(gam_obj)
```

```{r}
best_gam <- gam(formula = gpa ~ is_male + s(recent_math_score, 3) + recent_writing_score + 
    recent_science_score + parent_educationsome_high_school, 
    data = selected_train, trace = FALSE)

preds_best_gam <- best_gam %>% 
  predict(selected_test)

data.frame(RMSE = RMSE(preds_best_gam, selected_test$gpa), R2 = caret::R2(preds_best_gam, selected_test$gpa))
```

# 6: Supervised Classification - LDA + Logistic Regression

## 6.1 LDA Model Creation

```{r, include=FALSE}
# 6.1 LDA Model Creation

lda_model <- lda(gpa_desirable ~ ., data = selected_train |> select(-c("gpa", "gpa_letter")))

lda_preds <- predict(lda_model, newdata = selected_test |> select(-c("gpa", "gpa_letter")))

lda_class <- lda_preds$class
table(lda_class, selected_test$gpa_desirable)

summary(lda_model)
```

### LDA Accuracy

```{r}
accuracy <- mean(lda_class == selected_test$gpa_desirable)
print(accuracy)


#FIND DISCRIMINANT SCORE???

#IMPROVE???
```

## 6.2 LDA Model Evaluation

```{r}
# 6.2 LDA Model Evaluation

#LDA MODEL EVALUATION:
error_colors <- as.factor(selected_test$gpa_desirable == lda_class)

plot(pc2$x, col = error_colors, pch=20, cex=2)
legend("topright", legend = levels(error_colors),
       fill = 1:length(levels(error_colors)),
       pch = 20, title = "Reasonable Prediction")


predicted_probs <- lda_preds$posterior
colnames(predicted_probs) <- levels(selected_test$gpa_desirable)
```


## 6.3 Logistic Model Creation

```{r, include=FALSE}
# 6.3 Logistic Model Creation

#LOGISTIC REGRESSION

log_reg_model <- multinom(gpa_desirable ~ ., data = selected_train |> select(-c("gpa","gpa_letter")))
 
#log_reg_pred_probs <- predict(log_reg_model, newdata = test_cls, type = "probs")

log_reg_pred_class <- predict(log_reg_model, newdata = selected_test |> select(-c("gpa","gpa_letter")), type = "class")
 
 
log_reg_accuracy <- mean(log_reg_pred_class == selected_test$gpa_desirable)

```

## 6.4 Logistic Model Evaluation

### Plotting Logistic Regression Model Evaluation

```{r}
# 6.4 Logistic Model Evaluation

 #LOGISTIC MODEL EVALUATION:
error_colors <- as.factor(log_reg_pred_class == selected_test$gpa_desirable)
plot(pc2$x, col = error_colors, pch=20, cex=2)
legend("topright", legend = levels(error_colors),
       fill = 1:length(levels(error_colors)),
       pch = 20, title = "Reasonable Prediction")
```

### Logistic Regression Confusion Matrix

```{r}
table(log_reg_pred_class, selected_test$gpa_desirable)
```

### Logistic Regression Metrics 

```{r}
print(log_reg_accuracy)
```




# 7: Supervised Classification - Tree-Based Methods


## 7.1 Decision Tree

```{r, include=FALSE}
# 7.1 Decision Tree

train_input <- selected_train |> select(-gpa, -gpa_letter) 
decision_tree <- rpart(gpa_desirable ~ ., train_input)

summary(decision_tree)
```

### Decision Tree Plot

```{r}
rpart.plot(decision_tree)
```

## 7.2 Random Forest Creation

```{r, include=FALSE}
# 7.2 Random Forest Creation

random_forest_model <- randomForest(gpa_desirable~., train_input, ntree=500, importance=TRUE)
summary(random_forest_model)
```

## 7.3 Random Forest Evaluation

### Confusion Matrix

```{r}
# 7.3 Random Forest Evaluation

random_forest_model$confusion
```

### Variable Importance

```{r}
importance(random_forest_model)
```

### Variable Importance Plot

```{r}
varImpPlot(random_forest_model)
```



# 8: Supervised Classification - Support Vector Machines


## 8.1 Comparing SVM Models

```{r, include=FALSE}
# 8.1 Comparing SVM Models


# #SVM

# 
try = seq(1,1 #10
          ,by=1)

# 
# #ATTEMPTING TO DETERMINE THE RELATIONSHIP OF DATA (IE LINEAR,QUADRATIC,RADIAL)
# 
# #from PC, we expect Linear Model To Work Well
# 
# 
train <- selected_train |> select(-c("gpa", "gpa_letter"))
test <- selected_test |> select(-c("gpa", "gpa_letter"))


selected_train
best <- tune(svm, gpa_desirable ~ ., data = train, ranges = list(cost = try),kernel="linear")$best.parameters$cost
 linear_kernel_svm <- svm(gpa_desirable ~ .,data=train, cost=best, kernel="linear")
 lin_preds <- predict(linear_kernel_svm, test, type="class") != test$gpa_desirable
 lin_mis <- mean(lin_preds)

 best <- tune(svm, gpa_desirable ~ ., data = train, ranges = list(cost = try), kernel="radial")$best.parameters$cost

 radial_kernel_svm <- svm(gpa_desirable ~ .,data=train, cost=best, kernel="radial")
 rad_mis <- mean(predict(radial_kernel_svm, test, type="class") != test$gpa_desirable)
  
 
  best <- tune(svm, gpa_desirable ~ ., data = train, ranges = list(cost = try), kernel="polynomial",degree=2)$best.parameters$cost
 quadratic_kernel_svm <- svm(gpa_desirable ~ .,data=train, cost=best, kernel="polynomial",degree=2)
 quad_mis <- mean(predict(quadratic_kernel_svm, test, type="class") != test$gpa_desirable)
#
```


```{r}
tribble(~"LINEAR MIS", ~"RADIAL MIS", ~"QUAD MIS",
         lin_mis, rad_mis, quad_mis)
```

## 8.2 Display

```{r}
# 8.2 Display

 error_colors <- as.factor(!lin_preds)
 plot(pc2$x, col = error_colors, pch=20, cex=2)
 legend("topright", legend = levels(error_colors), 
        fill = 1:length(levels(error_colors)), 
        pch = 20, title = "Reasonable Prediction")
```


# 9 - Deep Learning

```{r, include=FALSE}
# 9.1 Loading Keras into the Workspace

#MOVEING THIS BACK UP
```


```{r, include=FALSE}
# 9.2 Creating the Network

# dim(selected_train)

X_nn_train <- as.matrix(selected_train |> select(-c("gpa_letter", gpa_desirable, gpa)))
Y_nn_train <- as.matrix(model.matrix(~gpa_letter -1, data=selected_train))

nn_keras <- keras_model_sequential() |>
  layer_dense(units=50, activation="relu", input_shape=6) |>
  layer_dropout(rate=0.1) |>
  layer_dense(units=25, activation="relu") |>
  layer_dense(units=length(unique(selected_train$gpa_letter)), activation="softmax")

nn_keras |>
  compile(
    loss="categorical_crossentropy",
    optimizer=optimizer_adam(),
    metrics=c("accuracy")
  )

nn_keras |>
  fit(
    X_nn_train, Y_nn_train,
    epochs=25,
    batch_size=10
  )
```

### Evaluating the Neural Network

```{r}
# 9.3 Evaluating the Network

X_nn_test <- as.matrix(selected_test |> select(-c("gpa_letter", gpa_desirable, gpa)))
Y_nn_test <- as.matrix(model.matrix(~gpa_letter -1, data=selected_test))
# summary(nn_keras)
pred_nn_keras_prob <- predict(nn_keras, X_nn_test, type="class")
colnames(pred_nn_keras_prob) <- c("gpa_letterA", "gpa_letterB", "gpa_letterC", "gpa_letterD", "gpa_letterF")
pred_nn_keras <- colnames(pred_nn_keras_prob)[apply(pred_nn_keras_prob, 1, which.max)]
Y_nn_test_label <- colnames(Y_nn_test)[apply(Y_nn_test, 1, which.max)]

acc_nn_keras <- mean(pred_nn_keras == Y_nn_test_label)
data.frame(Accuracy=acc_nn_keras)
```
# 10 - Conclusions



# OPTIONAL????????

# 11 - Future Research
- current emphasis on racial and economic factors + previous success
- expansion of analysis possible for geographic, ie between schools with varying urban/rural divide,
- possible opportunity to look at student mental health (IE: psycology, risk behaviours)
- maybe even something about access to technology, a metric about technical capacity ~ assuming technical skills give indication for success in increasingly technical school, idk, that last bit is a stretch





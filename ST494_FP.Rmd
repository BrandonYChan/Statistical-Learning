---
title: "Predicting and Understanding NBA Career Length"
author: "Brandon Chan + Lucas Duncan"
date: "2025-03-07"
output: html_document
---

```{r}
#NOTES TO SELF:::::::::
  
#-GPA is sum of 4 Classes, So any model cant use all 4

# Additionally we could make the classification stuff boolean (desirable or not)
#   SEARCH FOR "CLS-FIX"

```

## 2.0: Imports
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

if(! require("caret")){install.packages("caret")}
if(! require("tidyverse")){install.packages("tidyverse")}
if(! require("ISLR2")){install.packages("ISLR2")}
if(! require("boot")){install.packages("boot")}
if(! require("MASS") ){ install.packages("MASS") }
if(! require("leaps") ){ install.packages("leaps") }
if(! require("glmnet") ){ install.packages("glmnet") }
if(! require("pls") ){ install.packages("pls") }
if(! require("splines")) {install.packages("splines")}
if(! require("e1071")){ install.packages("e1071")}
if(! require("pROC")){ install.packages("pROC")}
if(! require("class")){ install.packages("class")}
if(! require("reshape2")){ install.packages("reshape2")}
if(! require("DAAG")){ install.packages("DAAG")}
if(! require("ROCR")){ install.packages("ROCR")}
if(! require("tree")){ install.packages("tree")}
if(! require("randomForest")){ install.packages("randomForest")}


library(tidyverse)
library(e1071)
library(MASS)
library(tidyverse)
library(caret)
library(leaps)
library(tidyr)
library(ggplot2)
library(glmnet)
library(ROCR)
library(pROC)
library(randomForest)
library(tree)
select <- dplyr::select
```


## Executive Summary and Contents
```{r}
contents <- tribble(
  ~Index, ~Category, ~Method,
  1, "Introduction", "Written",
  2, "Preprocessing", "Data Description and Encoding",
  3, "Unsupervised Learning/Analysis", "PCA and Clustering",
  4, "Preprocessing", "Variable Selection",
  5, "Supervised Regression", "Linear and Non-Linear Regression",
  6, "Supervised Classification", "LDA and Logistic Regression",
  7, "Supervised Classification", "Tree-Based Methods",
  8, "Supervised Classification", "Support Vector Machines",
  9, "Deep Learning", "Neural Networks",
  10, "Conclusion", "Written", 
)
contents
```
### 1: Introduction



### 2: Preprocessing

## 2.1 Extract Data
```{r, warning=FALSE}
location = "https://raw.githubusercontent.com/BrandonYChan/Statistical-Learning/main/Student_performance_10k.csv"

(df <- read_csv(location, show_col_types = FALSE, n_max=10000))
```

## 2.2 Clean Data
```{r}
df2 <- df |>
       rename(
         "has_subsidized_lunch" = "lunch",
         "has_prepared" = "test_preparation_course",
         "is_male" = "gender",
         "parent_education" = "parental_level_of_education",
         "gpa" = "total_score",
         "gpa_letter" = "grade",
         "recent_math_mark" = "math_score",
         "recent_writing_mark" = "writing_score",
         "recent_science_mark" = "science_score"
       ) |> 
       mutate(
         is_male = as.integer(if_else(is_male =="male",1,0)),
         race_ethnicity = as.factor(str_sub(df$race_ethnicity, -1, -1)),
         parent_education = as.factor(parent_education),
         gpa = gpa/4,
         
         gpa_letter = as.factor(if_else(gpa_letter == "Fail", "F", gpa_letter)),    #CLS-FIX
         
         gpa_desirable = as.factor(if_else(gpa_letter %in% c("Fail","D","C"), "UnDesirable", "Desirable"))
         
         
       ) |>
       select(-c("roll_no", "reading_score"))   #CLS-FIX
```

## 2.3 Interpolate NA values
```{r}
#Remove Any Duplicates
df3 <- distinct(df2)

#Replace Missing NUMERIC Values With Column Mean
if (any(is.na(df3))) {
  df3[] <- lapply(df3, function(x) {
    if (is.numeric(x)) {
      replace(x, is.na(x), mean(x, na.rm = TRUE))
    } else {
      x
    }
  })
}

#Dropping Missing Factor Values
df3 <- df3[complete.cases(df3[c("race_ethnicity", "parent_education", "gpa_desirable", "gpa_letter")]), ]   ###CLS-FIX
```

## 2.4 Display Variable Distributions
```{r}
#Display Numeric
df_numeric <- df3 |> select(where(is.numeric)) |> pivot_longer(cols = everything(), names_to = "variable", values_to = "value")
ggplot(df_numeric, aes(x = value)) + geom_histogram(bins = 30) +  facet_wrap(~ variable, scales = "free")  

#Display Factor
df_factor <- df3 |> select(where(is.factor)) |> pivot_longer(cols = everything(), names_to = "variable", values_to = "value") 
ggplot(df_factor, aes(x = value)) + geom_bar() + facet_wrap(~ variable, scales = "free") 
```

## 2.5 Format For Classification + Regression
```{r}

df4 <- df3 |> bind_cols(as.data.frame(model.matrix(~ `race_ethnicity` + `parent_education` - 1, data = df3))) |>
                 select(-c("race_ethnicity","parent_education"))
```


## 2.6 Class Balancing
```{r}




```



## 2.7 Split Into Train + Test Sets
```{r}
set.seed(10)
#Splits Rows
train_index <- sample(1:nrow(df4), nrow(df4)*0.8)

df_train <- df4[train_index,]
df_test <- df4[-train_index,]
```

### 3: Data Exploration with Unsupervised Methods 

## 3.1 Principal Component Analysis

## 3.2 Generate Principle Components Train + Test
```{r}

# Find Principal Components 
pc <- prcomp(df_train |> select(-c("gpa_desirable","gpa_letter")), scale=TRUE)   #CLS-FIX

pc2 <- prcomp(df_test |> select(-c("gpa_desirable","gpa_letter")), scale=TRUE)  #CLS-FIX

#Highlight Distribution Of TRAIN GPA in PC
gpa_colors <- as.factor(df_train$gpa_letter)
plot(pc$x, col = gpa_colors, pch=20, cex=2)
legend("topright", legend = levels(gpa_colors), 
       fill = 1:length(levels(gpa_colors)), 
       pch = 20, title = "GPA Letter")
```

## 3.3 Show Impact of Variables on the First Principle Component
```{r}
pc1 <- pc$rotation[,1]
(pc1_loading_df <- cbind(names(pc1), as_data_frame(pc1)) |>
                  rename("names" = "names(pc1)", "pc1_loading" = "value") |> 
                  arrange(desc(abs(pc1_loading))))
```

## 3.4 Show Proportion of Variance Explained (PVE)
```{r}
pve <- pc$sdev^2 / sum(pc$sdev^2)   

(pve_df <- as_data_frame(cbind(1:length(pve), pve)) %>% # Show PVE for each number of principal components
  mutate(cumulative_pve = cumsum(pve)) %>% # Cumulative PVE for each additional PC 
  rename("num_principal_components" = "V1"))
```

## 3.5 Show projections of data onto first 2 PCs
```{r}
biplot(pc, scale = 0, cex=0.4)
```
## 3.6 Generating Ideal K For Grouping
```{r}


```
## 3.7 K-Means Clustering on PC1 and PC2
```{r}

km <- kmeans(pc$x, 6, nstart=50)
plot(pc$x, col = (km$cluster +1), pch=20, cex=2)
```





### 4: Variable Selection


## 4.1 Comparing Metrics
```{r}
par(mfrow=c(1,3))

# Best subset selection
best_subset <- regsubsets(gpa~., df_train |> select(-c("gpa_letter", "gpa_desirable")), nvmax = ncol(df_train))
(bs_summary <- summary(best_subset))

# BIC plot
plot(seq_len(length(bs_summary$bic)), bs_summary$bic, type="b", xlab="Number of predictors", ylab="BIC")
points(which.min(bs_summary$bic), min(bs_summary$bic), col="red", cex=2, pch=20) 

# cp plot
plot(seq_len(length(bs_summary$cp)), bs_summary$cp, type="b", xlab="Number of predictors", ylab="CP",ylim=c(0,20))
points(which.min(bs_summary$cp), min(bs_summary$cp), col="red", cex=2, pch=20)
# lines(seq_along(bs_summary$cp), seq_along(bs_summary$cp)+1, col="gray", lty=2)

# Adjusted R^2 plot
plot(seq_len(length(bs_summary$adjr2)), bs_summary$adjr2, type="b", xlab="Number of predictors", ylab="R^2",ylim=c(0.45,1))
points(which.max(bs_summary$adjr2), max(bs_summary$adjr2), col="red", cex=2, pch=20)
```

## 4.2 Extracting Key Variables
```{r}
tibble(Best_BIC = which.min(bs_summary$bic), Best_Cp = which.min(bs_summary$cp), Best_Adjusted_Rsq = which.max(bs_summary$adjr2))


#BIC HEAVILY PENALIZED COMPLEXITY, BUT ADRSQ, AND CP BOTH SEEM TO BE OKAY WITH HIGHER, AND THE FACT LOTS OF THESE ARE ONE HOT VARIABLES MAKES ME THINK SLIGHTLY MORE IS OKAY, MAYBE WE JUST CHOOISE SOMETHING RANDOM FOR NOW, USE IT TO MOVE ON, THEN COME BACK

p = 6

chosen_model <- bs_summary$which[p, ]
chosen_predictors = gsub("`", "", names(chosen_model[chosen_model == TRUE]))
chosen_predictors

```

## 4.3 Filtering Data Columns
```{r}

#UPDATING TRAIN + TEST TO ONLY INCLUDE IMPORTANT VARIABLES
df_train <- df_train[, c("gpa","gpa_letter", "gpa_desirable", chosen_predictors[2:length(chosen_predictors)])]
df_test <- df_test[, c("gpa","gpa_letter", "gpa_desirable",chosen_predictors[2:length(chosen_predictors)])]

colnames(df_train) <- gsub(" ", "_", colnames(df_train)) 
colnames(df_train) <- gsub("'", "", colnames(df_train))

colnames(df_test) <- gsub(" ", "_", colnames(df_test)) 
colnames(df_test) <- gsub("'", "", colnames(df_test))



colnames(df_train)
```


### 5: Supervised Regression - Linear + Non Linear




## 5.1 Choosing Most Effective Linear Method
```{r}


train_x <- as.matrix(df_train[, !colnames(df_train) %in% c("gpa", "gpa_letter", "gpa_desirable")])  #FIX-CLS
train_y <- as.matrix(df_train$gpa)

test_x <- as.matrix(df_test[, !colnames(df_test) %in% c("gpa", "gpa_letter", "gpa_desirable")])  #FIX-CLS
test_y <- as.matrix(df_test$gpa)

#Linear Model
linear_model <- lm(gpa ~ ., data=df_train[,!colnames(df_train) %in% c("gpa_letter", "gpa_desirable")])  #FIX-CLS
linear_preds <- predict(linear_model, df_test[,!colnames(df_train) %in% c("gpa_letter", "gpa_desirable")])  #FIX-CLS
RMSE_linear = RMSE(test_y, linear_preds)
MAE_linear = MAE(test_y, linear_preds)

#LASSO:
optimal_lambda <- cv.glmnet(train_x, train_y, alpha=1)$lambda.min
lasso_model <- glmnet(train_x, train_y, alpha=1, lambda=optimal_lambda)
lasso_predictions <- predict(lasso_model, newx=test_x, s=optimal_lambda)
RMSE_lasso =  RMSE(test_y, lasso_predictions)
MAE_lasso = MAE(test_y, lasso_predictions)

#Ridge
optimal_lambda <- cv.glmnet(train_x, train_y, alpha=0)$lambda.min
ridge_model <- glmnet(train_x, train_y, alpha=0, lambda=optimal_lambda)
ridge_predictions <- predict(ridge_model, newx=test_x, s=optimal_lambda)
RMSE_ridge = RMSE(test_y, ridge_predictions)
MAE_ridge = MAE(test_y, ridge_predictions)

tribble(~"Unregularised Linear (RMSE/MAE)", ~"LASSO (RMSE/MAE)", ~"Ridge (RMSE/MAE)",
        RMSE_linear, RMSE_lasso, RMSE_ridge,
        MAE_linear, MAE_lasso, MAE_ridge
        )

```


## 5.2 Extracting Insight And Evaluating Chosen Linear Method
```{r}
# LASSO regression performs the best of these????????????//

#THEREFORE OUR BEST MODEL (AND INSIGHT FROM COEFFICIENTS COMES FROM LASSO)
coef(lasso_model)

accuracy_within_10 <- mean(abs(df_test$gpa -lasso_predictions) < 10)
accuracy_within_10


#LASSO MODEL EVALUATION:
error_colors <- as.factor(abs(df_test$gpa - lasso_predictions) < 10)
plot(pc2$x, col = error_colors, pch=20, cex=2)
legend("topright", legend = levels(error_colors), 
       fill = 1:length(levels(error_colors)), 
       pch = 20, title = "Reasonable Prediction")
```


## 5.3 .....

```{r}
# Polynomial Regression

# Natural Splines

# Generalized Additive Model (GAM)

```







### 6: Supervised Classification - LDA + Logistic Regression

## 6.1 LDA Model Creation
```{r}
lda_model <- lda(gpa_desirable ~ ., data = df_train |> select(-c("gpa", "gpa_letter")))

lda_preds <- predict(lda_model, newdata = df_test |> select(-c("gpa", "gpa_letter")))

lda_class <- lda_preds$class
table(lda_class, df_test$gpa_desirable)


accuracy <- mean(lda_class == df_test$gpa_desirable)
print(accuracy)
summary(lda_model)

#FIND DISCRIMINANT SCORE???

#IMPROVE???
```

## 6.2 LDA Model Evaluation
```{r}
#LDA MODEL EVALUATION:
error_colors <- as.factor(df_test$gpa_desirable == lda_class)

plot(pc2$x, col = error_colors, pch=20, cex=2)
legend("topright", legend = levels(error_colors),
       fill = 1:length(levels(error_colors)),
       pch = 20, title = "Reasonable Prediction")


##INTERESTING PLOT!!!!!!!!!!!!!!!!!!!!!
predicted_probs <- lda_preds$posterior
colnames(predicted_probs) <- levels(df_test$gpa_desirable)
```


## 6.3 Logistic Model Creation

```{r, include=FALSE}


#LOGISTIC REGRESSION

library(nnet)
 
log_reg_model <- multinom(gpa_desirable ~ ., data = df_train |> select(-c("gpa","gpa_letter")))
 
#log_reg_pred_probs <- predict(log_reg_model, newdata = test_cls, type = "probs")

log_reg_pred_class <- predict(log_reg_model, newdata = df_test |> select(-c("gpa","gpa_letter")), type = "class")
 
 
log_reg_accuracy <- mean(log_reg_pred_class == df_test$gpa_desirable)

```

## 6.4 Logistic Model Evaluation
```{r}
 #LOGISTIC MODEL EVALUATION:
error_colors <- as.factor(log_reg_pred_class == df_test$gpa_desirable)
plot(pc2$x, col = error_colors, pch=20, cex=2)
legend("topright", legend = levels(error_colors),
       fill = 1:length(levels(error_colors)),
       pch = 20, title = "Reasonable Prediction")

table(log_reg_pred_class, df_test$gpa_desirable)

print(log_reg_accuracy)
```




### 7: Supervised Classification - Tree-Based Methods


## 7.1 Decision Tree
```{r}
library(tree)



df_train
train_input <- df_train |> select(-gpa, -gpa_letter) 
decision_tree <- tree(gpa_desirable ~ ., train_input)


summary(decision_tree)
plot(decision_tree)
text(decision_tree)

```

## 7.2 Random Forest Creation

```{r}
random_forest_model <- randomForest(gpa_desirable~., train_input, ntree=500, importance=TRUE)
summary(random_forest_model)
```

## 7.3 Random Forest Evaluation
```{r}
random_forest_model$confusion
importance(random_forest_model)
varImpPlot(random_forest_model)
```



### 8: Supervised Classification - Support Vector Machines


## 8.1 Comparing SVM Models
```{r}
# # This won't fully run on my laptop (been going 10 mins) so it won't knit
# 

#IF YOU LAPTOP IS STRUGGLING WE DONT HAVE TO USE BEST WE CAN JUST USE SOME DEFAULT IE: 1
# (SEE BELOW)

# #SVM
 library(e1071)
# 
try = seq(1,1 #10
          ,by=1)

# 
# #ATTEMPTING TO DETERMINE THE RELATIONSHIP OF DATA (IE LINEAR,QUADRATIC,RADIAL)
# 
# #from PC, we expect Linear Model To Work Well
# 
# 
train <- df_train |> select(-c("gpa", "gpa_letter"))
test <- df_test |> select(-c("gpa", "gpa_letter"))


df_train
best <- tune(svm, gpa_desirable ~ ., data = train, ranges = list(cost = try),kernel="linear")$best.parameters$cost
 linear_kernel_svm <- svm(gpa_desirable ~ .,data=train, cost=best, kernel="linear")
 lin_preds <- predict(linear_kernel_svm, test, type="class") != test$gpa_desirable
 lin_mis <- mean(lin_preds)

 best <- tune(svm, gpa_desirable ~ ., data = train, ranges = list(cost = try), kernel="radial")$best.parameters$cost

 radial_kernel_svm <- svm(gpa_desirable ~ .,data=train, cost=best, kernel="radial")
 rad_mis <- mean(predict(radial_kernel_svm, test, type="class") != test$gpa_desirable)
  
 
  best <- tune(svm, gpa_desiable ~ ., data = train, ranges = list(cost = try), kernel="polynomial",degree=2)$best.parameters$cost
 quadratic_kernel_svm <- svm(gpa_desirable ~ .,data=train, cost=best, kernel="polynomial",degree=2)
 quad_mis <- mean(predict(quadratic_kernel_svm, test, type="class") != test$gpa_desirable)
# 
 tribble(~"LINEAR MIS", ~"RADIAL MIS", ~"QUAD MIS",
         lin_mis, rad_mis, quad_mis)
```

## 8.2 Display
```{r}
 error_colors <- as.factor(!lin_preds)
 plot(pc2$x, col = error_colors, pch=20, cex=2)
 legend("topright", legend = levels(error_colors), 
        fill = 1:length(levels(error_colors)), 
        pch = 20, title = "Reasonable Prediction")
```


### 9 - Deep Learning
```{r}



```





### 10 - Conclusions

